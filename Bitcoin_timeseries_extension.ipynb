{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "2jqbKj7egFCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Does scaling the data help for univariate/multivariate data? (e.g. getting all of the values between 0 & 1)\n",
        "  * Try doing this for a univariate model (e.g. model_1) and a multivariate model (e.g. model_6) and see if it effects model training or evaluation results.\n",
        "2. Get the most up to date data on Bitcoin, train a model & see how it goes (our data goes up to May 18 2021).\n",
        "  * You can download the Bitcoin historical data for free from coindesk.com/price/bitcoin and clicking “Export Data” -> “CSV”.\n",
        "3. For most of our models we used WINDOW_SIZE=7, but is there a better window size?\n",
        "  * Setup a series of experiments to find whether or not there’s a better window size.\n",
        "  * For example, you might train 10 different models with HORIZON=1 but with window sizes ranging from 2-12.\n",
        "4. Create a windowed dataset just like the ones we used for model_1 using tf.keras.preprocessing.timeseries_dataset_from_array() and retrain model_1 using the recreated dataset.\n",
        "5. For our multivariate modelling experiment, we added the Bitcoin block reward size as an extra feature to make our time series multivariate.\n",
        "  * Are there any other features you think you could add?\n",
        "  * If so, try it out, how do these affect the model?\n",
        "6. Make prediction intervals for future forecasts. To do so, one way would be to train an ensemble model on all of the data, make future forecasts with it and calculate the prediction intervals of the ensemble just like we did for model_8.\n",
        "7. For future predictions, try to make a prediction, retrain a model on the predictions, make a prediction, retrain a model, make a prediction, retrain a model, make a prediction (retrain a model each time a new prediction is made).\n",
        "8. Plot the results, how do they look compared to the future predictions where a model wasn’t retrained for every forecast (model_9)?\n",
        "Throughout this notebook, we’ve only tried algorithms we’ve handcrafted ourselves. But it’s worth seeing how a purpose built forecasting algorithm goes.\n",
        "  * Try out one of the extra algorithms listed in the modelling experiments part such as:\n",
        "  * Facebook’s Kats library - there are many models in here, remember the machine learning practioner’s motto: experiment, experiment, experiment.\n",
        "  * LinkedIn’s Greykite library"
      ],
      "metadata": {
        "id": "PGR-Gc8NmeP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the data and preprocessing"
      ],
      "metadata": {
        "id": "ruaRoc4ygKKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Bitcoin historical data from GitHub\n",
        "!wget 'https://raw.githubusercontent.com/DavAll22/Bitcoin_Forecast/main/bitcoin_2013-10-01_2023-06-14.csv'\n",
        "\n",
        "# Re-formatting to import using datetime:\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/bitcoin_2013-10-01_2023-06-14.csv',\n",
        "                 parse_dates=['Date'],\n",
        "                 index_col=['Date']) # parse the date column (column 1 is a datetime)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "gwL0faZjnT4U",
        "outputId": "13c282b0-82d7-472e-b2dd-a25cce7651be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-15 12:34:50--  https://raw.githubusercontent.com/DavAll22/Bitcoin_Forecast/main/bitcoin_2013-10-01_2023-06-14.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 339343 (331K) [text/plain]\n",
            "Saving to: ‘bitcoin_2013-10-01_2023-06-14.csv’\n",
            "\n",
            "bitcoin_2013-10-01_ 100%[===================>] 331.39K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2023-06-15 12:34:50 (86.9 MB/s) - ‘bitcoin_2013-10-01_2023-06-14.csv’ saved [339343/339343]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                Open      High       Low     Close        Volume    Market Cap\n",
              "Date                                                                          \n",
              "2023-06-12  25910.04  26300.08  25760.04  25910.03  3.377094e+10  5.042019e+11\n",
              "2023-06-11  25922.89  26081.61  25662.95  25887.59  3.018877e+10  5.018320e+11\n",
              "2023-06-10  25820.23  26147.84  25676.59  25920.86  3.299485e+10  5.011299e+11\n",
              "2023-06-09  26481.47  26500.50  25491.91  25842.84  3.586479e+10  5.009359e+11\n",
              "2023-06-08  26509.38  26766.28  26325.91  26489.46  2.879870e+10  5.143227e+11"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0797d353-598d-496c-96f6-6d1ef276c39b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Market Cap</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2023-06-12</th>\n",
              "      <td>25910.04</td>\n",
              "      <td>26300.08</td>\n",
              "      <td>25760.04</td>\n",
              "      <td>25910.03</td>\n",
              "      <td>3.377094e+10</td>\n",
              "      <td>5.042019e+11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-06-11</th>\n",
              "      <td>25922.89</td>\n",
              "      <td>26081.61</td>\n",
              "      <td>25662.95</td>\n",
              "      <td>25887.59</td>\n",
              "      <td>3.018877e+10</td>\n",
              "      <td>5.018320e+11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-06-10</th>\n",
              "      <td>25820.23</td>\n",
              "      <td>26147.84</td>\n",
              "      <td>25676.59</td>\n",
              "      <td>25920.86</td>\n",
              "      <td>3.299485e+10</td>\n",
              "      <td>5.011299e+11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-06-09</th>\n",
              "      <td>26481.47</td>\n",
              "      <td>26500.50</td>\n",
              "      <td>25491.91</td>\n",
              "      <td>25842.84</td>\n",
              "      <td>3.586479e+10</td>\n",
              "      <td>5.009359e+11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-06-08</th>\n",
              "      <td>26509.38</td>\n",
              "      <td>26766.28</td>\n",
              "      <td>26325.91</td>\n",
              "      <td>26489.46</td>\n",
              "      <td>2.879870e+10</td>\n",
              "      <td>5.143227e+11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0797d353-598d-496c-96f6-6d1ef276c39b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0797d353-598d-496c-96f6-6d1ef276c39b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0797d353-598d-496c-96f6-6d1ef276c39b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(by='Date', ascending = True, inplace = True)\n",
        "df[:10], df[-10:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5iyq4Srnidf",
        "outputId": "daab1d6c-7e53-4904-91cb-8228434d5d67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(              Open    High     Low   Close  Volume    Market Cap\n",
              " Date                                                            \n",
              " 2013-09-30  132.68  134.63  131.55  132.18     0.0  1.567875e+09\n",
              " 2013-10-01  132.05  133.59  102.25  114.13     0.0  1.501799e+09\n",
              " 2013-10-02  114.45  123.63  111.82  123.63     0.0  1.412675e+09\n",
              " 2013-10-03  123.41  130.09  123.41  129.01     0.0  1.500255e+09\n",
              " 2013-10-04  128.63  130.44  128.03  128.55     0.0  1.522529e+09\n",
              " 2013-10-05  128.36  129.66  126.15  129.00     0.0  1.511180e+09\n",
              " 2013-10-06  129.43  130.27  126.36  126.94     0.0  1.518677e+09\n",
              " 2013-10-07  126.74  127.47  124.71  126.00     0.0  1.487193e+09\n",
              " 2013-10-08  125.85  131.75  125.58  130.69     0.0  1.524165e+09\n",
              " 2013-10-09  130.67  131.50  129.26  130.59     0.0  1.542156e+09,\n",
              "                 Open      High       Low     Close        Volume    Market Cap\n",
              " Date                                                                          \n",
              " 2023-06-03  27063.05  27384.43  26978.50  27104.41  2.001703e+10  5.266992e+11\n",
              " 2023-06-04  27092.18  27123.39  25447.67  25730.52  3.290454e+10  5.127312e+11\n",
              " 2023-06-05  25732.42  27278.06  25438.67  27200.04  6.527817e+10  5.062225e+11\n",
              " 2023-06-06  27205.09  27316.85  26141.01  26359.59  5.373495e+10  5.174071e+11\n",
              " 2023-06-07  26341.33  26762.75  26235.14  26493.46  3.797578e+10  5.132887e+11\n",
              " 2023-06-08  26509.38  26766.28  26325.91  26489.46  2.879870e+10  5.143227e+11\n",
              " 2023-06-09  26481.47  26500.50  25491.91  25842.84  3.586479e+10  5.009359e+11\n",
              " 2023-06-10  25820.23  26147.84  25676.59  25920.86  3.299485e+10  5.011299e+11\n",
              " 2023-06-11  25922.89  26081.61  25662.95  25887.59  3.018877e+10  5.018320e+11\n",
              " 2023-06-12  25910.04  26300.08  25760.04  25910.03  3.377094e+10  5.042019e+11)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only want the closing price per day\n",
        "bitcoin_prices = pd.DataFrame(df['Close']).rename(columns={'Close': 'Price'})\n",
        "bitcoin_prices.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "gHjX-TNHnq60",
        "outputId": "0300ea2c-4735-4f8c-e257-93448ca41287"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Price\n",
              "Date              \n",
              "2013-09-30  132.18\n",
              "2013-10-01  114.13\n",
              "2013-10-02  123.63\n",
              "2013-10-03  129.01\n",
              "2013-10-04  128.55"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c2e401c0-3f0b-40dc-ab9c-a4fc2aab5681\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-09-30</th>\n",
              "      <td>132.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-01</th>\n",
              "      <td>114.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-02</th>\n",
              "      <td>123.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-03</th>\n",
              "      <td>129.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-04</th>\n",
              "      <td>128.55</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2e401c0-3f0b-40dc-ab9c-a4fc2aab5681')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c2e401c0-3f0b-40dc-ab9c-a4fc2aab5681 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c2e401c0-3f0b-40dc-ab9c-a4fc2aab5681');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the data in an array\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "timesteps = bitcoin_prices.index.to_numpy()\n",
        "prices = bitcoin_prices['Price'].to_numpy()\n",
        "\n",
        "# Instantiating sklearn MinMaxScalar\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scalar = MinMaxScaler()"
      ],
      "metadata": {
        "id": "Ygdg3Ehgnxoz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function to view NumPy arrays as windows\n",
        "\n",
        "def get_labelled_windows(x , horizon):\n",
        "  return x[:, :-horizon] ,x[: , -horizon:]\n",
        "\n",
        "\n",
        "def make_windows_scaled(x, window_size=7, horizon=1):\n",
        "  \"\"\"\n",
        "  Turns a 1D array into a 2D array of sequential windows of window_size. Also applies the standard scalar\n",
        "  \"\"\"\n",
        "  scalar.fit(np.expand_dims(x , axis =1))\n",
        "  scaled_x = scalar.transform(np.expand_dims(x , axis = 1))\n",
        "  scaled_x = np.squeeze(scaled_x)\n",
        "\n",
        "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
        "  window_indexes = window_step + np.expand_dims(np.arange(len(scaled_x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of size window_size\n",
        "  windowed_array = scaled_x[window_indexes]\n",
        "  windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
        "\n",
        "  return windows, labels\n",
        "\n",
        "\n",
        "# Make the splits\n",
        "def make_train_test_splits(windows , labels , test_split = 0.2):\n",
        "  split_size = int(len(windows) * (1 - test_split))\n",
        "  train_windows = windows[:split_size]\n",
        "  train_labels = labels[:split_size]\n",
        "  test_windows = windows[split_size:]\n",
        "  test_labels = labels[split_size:]\n",
        "\n",
        "  return train_windows ,  test_windows ,train_labels,  test_labels"
      ],
      "metadata": {
        "id": "0f0z6NGMoKU3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Scaling the data for univariate & multivariate data"
      ],
      "metadata": {
        "id": "5qGc7U5hogCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All values between 0 and 1 - does this effect model training or evaluation?\n",
        "\n",
        "Univariate = model_1\n",
        "\n",
        "Multivariate = model_6"
      ],
      "metadata": {
        "id": "WZOAChbgoldk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model_1 univariate"
      ],
      "metadata": {
        "id": "gMqZ5m4zphiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 1 (Horizon = 1 , Window_size = 7)\n",
        "HORIZON = 1\n",
        "WINDOW_SIZE = 7\n",
        "\n",
        "\n",
        "full_windows , full_labels = make_windows_scaled(prices , window_size = WINDOW_SIZE , horizon = HORIZON)\n",
        "full_windows.shape , full_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbd4Dj1pok98",
        "outputId": "d259eacf-7c2f-4568-ec2f-d46ebe7746ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3536, 7), (3536, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at few examples of how price is scaled\n",
        "for i in range(3):\n",
        "  print(f'Window: {full_windows[i]} --> Label {full_labels[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5QFIENxo8hf",
        "outputId": "843de0f9-57a8-4ce2-ffb8-666a755f57c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window: [0.00026786 0.         0.00014098 0.00022082 0.00021399 0.00022067\n",
            " 0.0001901 ] --> Label [0.00017615]\n",
            "Window: [0.         0.00014098 0.00022082 0.00021399 0.00022067 0.0001901\n",
            " 0.00017615] --> Label [0.00024575]\n",
            "Window: [0.00014098 0.00022082 0.00021399 0.00022067 0.0001901  0.00017615\n",
            " 0.00024575] --> Label [0.00024426]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making train and test splits\n",
        "train_windows , test_windows , train_labels , test_labels = make_train_test_splits(full_windows , full_labels)\n",
        "len(train_windows), len(test_windows), len(train_labels), len(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h-6zlEdpP_S",
        "outputId": "acd0d90c-d73f-4902-89a4-f592bfbd7066"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2828, 708, 2828, 708)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the Model 1\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Construct the model\n",
        "model_1 = tf.keras.Sequential([\n",
        "  layers.Dense(128, activation= 'relu') ,\n",
        "  layers.Dense(HORIZON , activation = 'linear')\n",
        "])\n",
        "\n",
        "# Compiling the model\n",
        "model_1.compile(loss = 'mae' ,\n",
        "                optimizer = tf.keras.optimizers.Adam() ,\n",
        "                metrics = ['mae'])\n",
        "\n",
        "# Fit the model\n",
        "model_1_history = model_1.fit(x = train_windows ,\n",
        "                              y = train_labels ,\n",
        "                              epochs = 100 , batch_size = 128 , verbose = 0 ,\n",
        "                              validation_data = (test_windows , test_labels))"
      ],
      "metadata": {
        "id": "pyjkC-aNpYPN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test data\n",
        "model_1.evaluate(test_windows , test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGGwxWrmpnFV",
        "outputId": "c24fa48b-3d52-4acc-fe8a-a2d60f3b962b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0140 - mae: 0.0140\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.014037152752280235, 0.014037152752280235]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions\n",
        "model_1_preds = tf.squeeze(model_1.predict(test_windows))\n",
        "model_1_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLMJEO0ApwLT",
        "outputId": "97d0f802-aa0e-4c3f-8ead-70ebb47b6444"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
              "array([0.49792528, 0.49460873, 0.4930452 , 0.4797451 , 0.49008167,\n",
              "       0.48828346, 0.4976998 , 0.4876391 , 0.47418615, 0.4763597 ],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model_6 multivariate"
      ],
      "metadata": {
        "id": "Fog14uhtpkVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block reward values\n",
        "block_reward_1 = 50 # 3 January 2009\n",
        "block_reward_2 = 25 # 28 November 2012\n",
        "block_reward_3 = 12.5 # 9 July 2016\n",
        "block_reward_4 = 6.25 # 11 May 2020\n",
        "\n",
        "# Block reward dates (datetime form of the above date stamps)\n",
        "block_reward_2_datetime = np.datetime64(\"2012-11-28\")\n",
        "block_reward_3_datetime = np.datetime64(\"2016-07-09\")\n",
        "block_reward_4_datetime = np.datetime64(\"2020-05-11\")\n",
        "\n",
        "# Get date indexes for when to add in different block dates\n",
        "block_reward_2_days = (block_reward_3_datetime - bitcoin_prices.index[0]).days\n",
        "block_reward_3_days = (block_reward_4_datetime - bitcoin_prices.index[0]).days\n",
        "block_reward_2_days, block_reward_3_days\n",
        "\n",
        "# Add block_reward column\n",
        "bitcoin_prices_block = bitcoin_prices.copy()\n",
        "bitcoin_prices_block[\"block_reward\"] = None\n",
        "\n",
        "# Set values of block_reward column (it's the last column hence -1 indexing on iloc)\n",
        "bitcoin_prices_block.iloc[:block_reward_2_days, -1] = block_reward_2\n",
        "bitcoin_prices_block.iloc[block_reward_2_days:block_reward_3_days, -1] = block_reward_3\n",
        "bitcoin_prices_block.iloc[block_reward_3_days:, -1] = block_reward_4\n",
        "bitcoin_prices_block.head()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "a-3Mka_spmkv",
        "outputId": "831687b2-ea0d-4478-9f39-53fdb9816f61"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Price block_reward\n",
              "Date                           \n",
              "2013-09-30  132.18           25\n",
              "2013-10-01  114.13           25\n",
              "2013-10-02  123.63           25\n",
              "2013-10-03  129.01           25\n",
              "2013-10-04  128.55           25"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5fdfbe7-d7e7-4edb-b539-b1260091485b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "      <th>block_reward</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-09-30</th>\n",
              "      <td>132.18</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-01</th>\n",
              "      <td>114.13</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-02</th>\n",
              "      <td>123.63</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-03</th>\n",
              "      <td>129.01</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-04</th>\n",
              "      <td>128.55</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5fdfbe7-d7e7-4edb-b539-b1260091485b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f5fdfbe7-d7e7-4edb-b539-b1260091485b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f5fdfbe7-d7e7-4edb-b539-b1260091485b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy of the Bitcoin historical data with block reward feature\n",
        "bitcoin_prices_windowed = bitcoin_prices_block.copy()\n",
        "\n",
        "# Add windowed columns\n",
        "for i in range(WINDOW_SIZE):\n",
        "  bitcoin_prices_windowed[f\"Price+{i+1}\"] = bitcoin_prices_windowed[\"Price\"].shift(periods=i+1)\n",
        "bitcoin_prices_windowed.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "pRDKIP1vp2dS",
        "outputId": "278f0856-9b55-449d-e1ea-234f5ba561f9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Price block_reward  Price+1  Price+2  Price+3  Price+4  Price+5  \\\n",
              "Date                                                                           \n",
              "2013-09-30  132.18           25      NaN      NaN      NaN      NaN      NaN   \n",
              "2013-10-01  114.13           25   132.18      NaN      NaN      NaN      NaN   \n",
              "2013-10-02  123.63           25   114.13   132.18      NaN      NaN      NaN   \n",
              "2013-10-03  129.01           25   123.63   114.13   132.18      NaN      NaN   \n",
              "2013-10-04  128.55           25   129.01   123.63   114.13   132.18      NaN   \n",
              "2013-10-05  129.00           25   128.55   129.01   123.63   114.13   132.18   \n",
              "2013-10-06  126.94           25   129.00   128.55   129.01   123.63   114.13   \n",
              "2013-10-07  126.00           25   126.94   129.00   128.55   129.01   123.63   \n",
              "2013-10-08  130.69           25   126.00   126.94   129.00   128.55   129.01   \n",
              "2013-10-09  130.59           25   130.69   126.00   126.94   129.00   128.55   \n",
              "\n",
              "            Price+6  Price+7  \n",
              "Date                          \n",
              "2013-09-30      NaN      NaN  \n",
              "2013-10-01      NaN      NaN  \n",
              "2013-10-02      NaN      NaN  \n",
              "2013-10-03      NaN      NaN  \n",
              "2013-10-04      NaN      NaN  \n",
              "2013-10-05      NaN      NaN  \n",
              "2013-10-06   132.18      NaN  \n",
              "2013-10-07   114.13   132.18  \n",
              "2013-10-08   123.63   114.13  \n",
              "2013-10-09   129.01   123.63  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-75b3e67c-91c9-4eef-993c-fe2f33947907\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "      <th>block_reward</th>\n",
              "      <th>Price+1</th>\n",
              "      <th>Price+2</th>\n",
              "      <th>Price+3</th>\n",
              "      <th>Price+4</th>\n",
              "      <th>Price+5</th>\n",
              "      <th>Price+6</th>\n",
              "      <th>Price+7</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-09-30</th>\n",
              "      <td>132.18</td>\n",
              "      <td>25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-01</th>\n",
              "      <td>114.13</td>\n",
              "      <td>25</td>\n",
              "      <td>132.18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-02</th>\n",
              "      <td>123.63</td>\n",
              "      <td>25</td>\n",
              "      <td>114.13</td>\n",
              "      <td>132.18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-03</th>\n",
              "      <td>129.01</td>\n",
              "      <td>25</td>\n",
              "      <td>123.63</td>\n",
              "      <td>114.13</td>\n",
              "      <td>132.18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-04</th>\n",
              "      <td>128.55</td>\n",
              "      <td>25</td>\n",
              "      <td>129.01</td>\n",
              "      <td>123.63</td>\n",
              "      <td>114.13</td>\n",
              "      <td>132.18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-05</th>\n",
              "      <td>129.00</td>\n",
              "      <td>25</td>\n",
              "      <td>128.55</td>\n",
              "      <td>129.01</td>\n",
              "      <td>123.63</td>\n",
              "      <td>114.13</td>\n",
              "      <td>132.18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-06</th>\n",
              "      <td>126.94</td>\n",
              "      <td>25</td>\n",
              "      <td>129.00</td>\n",
              "      <td>128.55</td>\n",
              "      <td>129.01</td>\n",
              "      <td>123.63</td>\n",
              "      <td>114.13</td>\n",
              "      <td>132.18</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-07</th>\n",
              "      <td>126.00</td>\n",
              "      <td>25</td>\n",
              "      <td>126.94</td>\n",
              "      <td>129.00</td>\n",
              "      <td>128.55</td>\n",
              "      <td>129.01</td>\n",
              "      <td>123.63</td>\n",
              "      <td>114.13</td>\n",
              "      <td>132.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-08</th>\n",
              "      <td>130.69</td>\n",
              "      <td>25</td>\n",
              "      <td>126.00</td>\n",
              "      <td>126.94</td>\n",
              "      <td>129.00</td>\n",
              "      <td>128.55</td>\n",
              "      <td>129.01</td>\n",
              "      <td>123.63</td>\n",
              "      <td>114.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-09</th>\n",
              "      <td>130.59</td>\n",
              "      <td>25</td>\n",
              "      <td>130.69</td>\n",
              "      <td>126.00</td>\n",
              "      <td>126.94</td>\n",
              "      <td>129.00</td>\n",
              "      <td>128.55</td>\n",
              "      <td>129.01</td>\n",
              "      <td>123.63</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75b3e67c-91c9-4eef-993c-fe2f33947907')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-75b3e67c-91c9-4eef-993c-fe2f33947907 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-75b3e67c-91c9-4eef-993c-fe2f33947907');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create X & y, remove the NaN's and convert to float32 to prevent TensorFlow errors\n",
        "X = bitcoin_prices_windowed.dropna().drop(\"Price\", axis=1).astype(np.float32)\n",
        "y = bitcoin_prices_windowed.dropna()[\"Price\"].astype(np.float32)\n",
        "X.head(), y.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57KGg-idp-Ys",
        "outputId": "8d3e558a-9d3f-4b69-a8d0-59b088dd38f4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(            block_reward     Price+1     Price+2     Price+3     Price+4  \\\n",
              " Date                                                                       \n",
              " 2013-10-07          25.0  126.940002  129.000000  128.550003  129.009995   \n",
              " 2013-10-08          25.0  126.000000  126.940002  129.000000  128.550003   \n",
              " 2013-10-09          25.0  130.690002  126.000000  126.940002  129.000000   \n",
              " 2013-10-10          25.0  130.589996  130.690002  126.000000  126.940002   \n",
              " 2013-10-11          25.0  130.899994  130.589996  130.690002  126.000000   \n",
              " \n",
              "                Price+5     Price+6     Price+7  \n",
              " Date                                            \n",
              " 2013-10-07  123.629997  114.129997  132.179993  \n",
              " 2013-10-08  129.009995  123.629997  114.129997  \n",
              " 2013-10-09  128.550003  129.009995  123.629997  \n",
              " 2013-10-10  129.000000  128.550003  129.009995  \n",
              " 2013-10-11  126.940002  129.000000  128.550003  ,\n",
              " Date\n",
              " 2013-10-07    126.000000\n",
              " 2013-10-08    130.690002\n",
              " 2013-10-09    130.589996\n",
              " 2013-10-10    130.899994\n",
              " 2013-10-11    135.190002\n",
              " Name: Price, dtype: float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling the X data\n",
        "X_scaled = scalar.fit_transform(X)\n",
        "y_scaled = scalar.fit_transform(np.expand_dims(y , axis = 1))\n",
        "y_scaled = np.squeeze(y_scaled)"
      ],
      "metadata": {
        "id": "sxGrXx3JqIGf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make train and test set splits of the scaled data\n",
        "split_size = int(len(X) * 0.8)\n",
        "X_train, y_train = X_scaled[:split_size], y_scaled[:split_size]\n",
        "X_test, y_test = X_scaled[split_size:], y_scaled[split_size:]\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbA36RYlqhVm",
        "outputId": "34e72e46-3c6a-4231-9099-87084c773c23"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2828, 2828, 708, 708)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a Multivariate time series model and fitting it\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model_6 = tf.keras.Sequential([\n",
        "  layers.Dense(128 , activation= 'relu'),\n",
        "  layers.Dense(HORIZON)\n",
        "])\n",
        "\n",
        "model_6.compile(loss = 'mae' ,\n",
        "                optimizer = tf.keras.optimizers.Adam())\n",
        "\n",
        "model_6_history = model_6.fit(X_train , y_train ,\n",
        "                              epochs = 100 ,\n",
        "                              verbose = 0 , batch_size = 128,\n",
        "                              validation_data = (X_test , y_test))"
      ],
      "metadata": {
        "id": "WJwViLS5qpr4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model 6\n",
        "model_6.evaluate(X_test , y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K32Tw38yqyYW",
        "outputId": "78d8a298-12d9-4de0-ae87-8f4e350b8e9a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0127\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.012700935825705528"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions\n",
        "model_1_preds = tf.squeeze(model_1.predict(test_windows))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XS7X5t9q0b9",
        "outputId": "7b575807-2166-43bb-af0a-2feca612e0f6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Window size experimentation"
      ],
      "metadata": {
        "id": "e41WQBNiq8_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training 10 diferent models with widnow sizes from 2-13 and 1 horizon."
      ],
      "metadata": {
        "id": "em4WniQAuDae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing a evaluation function based on the preds and targets\n",
        "def evaluate_preds(y_true , y_pred):\n",
        "\n",
        "  # Casting the values to float32\n",
        "  y_true = tf.cast(y_true , tf.float32)\n",
        "  y_pred = tf.cast(y_pred , tf.float32)\n",
        "\n",
        "\n",
        "  # Calculate the metrics\n",
        "  mae = tf.keras.metrics.mean_absolute_error(y_true , y_pred)\n",
        "  mse = tf.keras.metrics.mean_squared_error(y_true , y_pred)\n",
        "  rmse = tf.sqrt(mse)\n",
        "  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true , y_pred)\n",
        "\n",
        "  # For longer horizons\n",
        "  if mae.ndim > 0:\n",
        "    mae = tf.reduce_sum(mae)\n",
        "    mse = tf.reduce_sum(mse)\n",
        "    rmse = tf.reduce_sum(rmse)\n",
        "    mape = tf.reduce_sum(mape)\n",
        "\n",
        "  return {'mae' : mae.numpy() ,\n",
        "          'mse': mse.numpy() ,\n",
        "          'rmse': rmse.numpy() ,\n",
        "          'mape': mape.numpy() }"
      ],
      "metadata": {
        "id": "DMGrGEgZuB5Z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to make windows for whole time series data using numpy array indexing\n",
        "# Same function as before, but without scaling\n",
        "\n",
        "def make_windows(x, window_size=7, horizon=1):\n",
        "  \"\"\"\n",
        "  Turns a 1D array into a 2D array of sequential windows of window_size.\n",
        "  \"\"\"\n",
        "  # 1. Create a window of specific window_size (add horizon on the end for later labelling)\n",
        "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
        "\n",
        "  # 2. Create 2D array of multiple window steps (minus 1 to account for 0 indexing)\n",
        "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T\n",
        "\n",
        "  # 3. Index on target array (time series) with 2D array of multiple window steps\n",
        "  windowed_array = x[window_indexes]\n",
        "\n",
        "  # 4. Get the lebeleld windows\n",
        "  windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
        "\n",
        "  return windows, labels"
      ],
      "metadata": {
        "id": "oLNtOHU2vADe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing a for loop to iterate over the Window size and build 10 different models\n",
        "\n",
        "# 10 Different models with window size ranging from (2 - 12) and store the results\n",
        "model_results_list = []\n",
        "\n",
        "from tqdm import tqdm # Progress bar updates 1 step when 1 model has been completed with a window size\n",
        "for size in tqdm(range(2,12)):\n",
        "  HORIZON = 1\n",
        "  WINDOW_SIZE = size\n",
        "\n",
        "  # Making window and labels\n",
        "  full_windows , full_labels = make_windows(prices, window_size= WINDOW_SIZE , horizon= HORIZON)\n",
        "\n",
        "\n",
        "  # Splitting the data in train and test\n",
        "  train_windows ,  test_windows ,train_labels,  test_labels = make_train_test_splits(full_windows , full_labels)\n",
        "\n",
        "\n",
        "  # Building a simple dense model\n",
        "  input = layers.Input(shape = (WINDOW_SIZE ,) , name = 'Input_layer')\n",
        "  x = layers.Dense(128 , activation= 'relu')(input)\n",
        "  output = layers.Dense(HORIZON , activation= 'linear')(x)\n",
        "\n",
        "  # Packing into a model\n",
        "  model = tf.keras.Model(input , output , name = f'model_windowed_{size}')\n",
        "\n",
        "  # Compiling and fitting the model\n",
        "  model.compile(loss = 'mae' , optimizer = 'adam' , metrics = 'mae')\n",
        "\n",
        "  model.fit(train_windows , train_labels ,\n",
        "            epochs = 100 , verbose = 0 ,\n",
        "            batch_size = 128 ,\n",
        "            validation_data = (test_windows , test_labels))\n",
        "\n",
        "\n",
        "  # Making predictions\n",
        "  preds_ = model.predict(test_windows)\n",
        "  y_preds = tf.squeeze(preds_)\n",
        "\n",
        "  results = evaluate_preds(tf.squeeze(test_labels) , y_preds)\n",
        "  model_results_list.append(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfbdIVNTuMgj",
        "outputId": "9eb3a5f4-487a-4c8d-9e6f-fb24c0dc0656"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [00:21<03:11, 21.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:33<02:06, 15.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:46<01:41, 14.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [01:07<01:43, 17.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [01:29<01:34, 18.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [01:50<01:18, 19.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [02:01<00:50, 16.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [02:11<00:29, 14.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [02:33<00:16, 16.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [02:44<00:00, 16.41s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Below are the 10 different models result\n",
        "model_results_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDm4Rkwn0kIh",
        "outputId": "fe4d8d3b-03dc-4a3f-c91d-c3eb3f4d5141"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'mae': 778.14935, 'mse': 1398654.5, 'rmse': 1182.6472, 'mape': 2.305723},\n",
              " {'mae': 952.8092, 'mse': 1773946.6, 'rmse': 1331.8959, 'mape': 2.8206265},\n",
              " {'mae': 907.0029, 'mse': 1700009.2, 'rmse': 1303.844, 'mape': 2.728623},\n",
              " {'mae': 817.8255, 'mse': 1488321.9, 'rmse': 1219.9679, 'mape': 2.4252703},\n",
              " {'mae': 905.08905, 'mse': 1695910.8, 'rmse': 1302.2714, 'mape': 2.7207751},\n",
              " {'mae': 1032.6215, 'mse': 1995873.5, 'rmse': 1412.7539, 'mape': 3.0712655},\n",
              " {'mae': 785.4647, 'mse': 1419487.2, 'rmse': 1191.4224, 'mape': 2.3141468},\n",
              " {'mae': 800.4836, 'mse': 1461144.2, 'rmse': 1208.778, 'mape': 2.3669927},\n",
              " {'mae': 790.07947, 'mse': 1432437.5, 'rmse': 1196.8447, 'mape': 2.3238575},\n",
              " {'mae': 1255.436, 'mse': 2702082.2, 'rmse': 1643.8011, 'mape': 3.77065}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function does not evaluate using the best results in training epochs (no callback used like in the last workbook) - here is making predictions using the last epoch's results.\n",
        "\n",
        "Appears a window size of 3 gives the best results regardless"
      ],
      "metadata": {
        "id": "G4rCpS3z2NRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Making windowed datasets using `tf.keras.preprocessing.timeseries_dataset_from_array()`"
      ],
      "metadata": {
        "id": "aeUEHi9a01T4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-making model_1 but using alternative method"
      ],
      "metadata": {
        "id": "WFsVsb9j3CYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE = 7\n",
        "HORIZON = 1"
      ],
      "metadata": {
        "id": "KsZyzOYm2-lv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the splits\n",
        "def make_train_test_splits(windows , labels , test_split = 0.2):\n",
        "  split_size = int(len(windows) * (1 - test_split))\n",
        "  train_windows = windows[:split_size]\n",
        "  train_labels = labels[:split_size]\n",
        "  test_windows = windows[split_size:]\n",
        "  test_labels = labels[split_size:]\n",
        "\n",
        "  return train_windows ,  test_windows ,train_labels,  test_labels\n",
        ""
      ],
      "metadata": {
        "id": "UBLAvXTD3Kg7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    data = prices , targets = prices , sequence_length = WINDOW_SIZE , sequence_stride = HORIZON,\n",
        "    batch_size = 128\n",
        ")"
      ],
      "metadata": {
        "id": "ZTHXwgAx3BcO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size , test_size = int(0.8 * len(ds)) ,int(0.2 * len(ds))"
      ],
      "metadata": {
        "id": "4Ks5iV2e3RUa"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dataset splits from the windows\n",
        "train_ds = ds.take(train_size)\n",
        "test_ds = ds.skip(train_size).take(test_size)"
      ],
      "metadata": {
        "id": "s-BdKTO33VA5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x , y in train_ds.take(1):\n",
        "  print(x[:2] , y[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjpstGs83wJz",
        "outputId": "ec888752-b124-4d29-fe7a-8b955f0791d9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[132.18 114.13 123.63 129.01 128.55 129.   126.94]\n",
            " [114.13 123.63 129.01 128.55 129.   126.94 126.  ]], shape=(2, 7), dtype=float64) tf.Tensor([132.18 114.13], shape=(2,), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x , y in test_ds.take(1):\n",
        "  print(x[:2] , y[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVR-b5Pt31Tb",
        "outputId": "fce77a86-ac8b-4ffd-ba9d-e1c2d835bd4e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[38082.08       35577.47       35635.38       35648.51716347\n",
            "  31606.39       32366.97       33646.15      ]\n",
            " [35577.47       35635.38       35648.51716347 31606.39\n",
            "  32366.97       33646.15       34580.351177  ]], shape=(2, 7), dtype=float64) tf.Tensor([38082.08 35577.47], shape=(2,), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the Model 1 with the updated data\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Building a simple dense model\n",
        "input = layers.Input(shape = (WINDOW_SIZE ,) , name = 'Input_layer' , dtype = tf.float32)\n",
        "x = layers.Dense(128 , activation= 'relu')(input)\n",
        "output = layers.Dense(HORIZON , activation= 'linear')(x)\n",
        "\n",
        "# Packing into a model\n",
        "model = tf.keras.Model(input , output)\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(loss = 'mae' ,\n",
        "                optimizer = tf.keras.optimizers.Adam() ,\n",
        "                metrics = ['mae'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(train_ds ,\n",
        "          epochs = 100 , verbose = 1 ,\n",
        "          validation_data = test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1upzjO5370M",
        "outputId": "ff48830a-7a22-4836-fca9-29f2bca48fad"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "22/22 [==============================] - 2s 38ms/step - loss: 353.3478 - mae: 353.3478 - val_loss: 1162.4128 - val_mae: 1162.4128\n",
            "Epoch 2/100\n",
            "22/22 [==============================] - 1s 34ms/step - loss: 403.8473 - mae: 403.8473 - val_loss: 920.4825 - val_mae: 920.4825\n",
            "Epoch 3/100\n",
            "22/22 [==============================] - 1s 48ms/step - loss: 612.2552 - mae: 612.2552 - val_loss: 1736.2594 - val_mae: 1736.2594\n",
            "Epoch 4/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 338.1854 - mae: 338.1854 - val_loss: 870.6381 - val_mae: 870.6381\n",
            "Epoch 5/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 522.4657 - mae: 522.4657 - val_loss: 1205.4996 - val_mae: 1205.4996\n",
            "Epoch 6/100\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 225.8116 - mae: 225.8116 - val_loss: 835.3959 - val_mae: 835.3959\n",
            "Epoch 7/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 221.2591 - mae: 221.2591 - val_loss: 918.6849 - val_mae: 918.6849\n",
            "Epoch 8/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 228.3367 - mae: 228.3367 - val_loss: 936.1410 - val_mae: 936.1410\n",
            "Epoch 9/100\n",
            "22/22 [==============================] - 1s 34ms/step - loss: 223.5751 - mae: 223.5751 - val_loss: 933.0825 - val_mae: 933.0825\n",
            "Epoch 10/100\n",
            "22/22 [==============================] - 2s 94ms/step - loss: 196.3047 - mae: 196.3047 - val_loss: 795.7133 - val_mae: 795.7133\n",
            "Epoch 11/100\n",
            "22/22 [==============================] - 1s 50ms/step - loss: 178.3318 - mae: 178.3318 - val_loss: 675.4566 - val_mae: 675.4566\n",
            "Epoch 12/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 170.2129 - mae: 170.2129 - val_loss: 627.9723 - val_mae: 627.9723\n",
            "Epoch 13/100\n",
            "22/22 [==============================] - 1s 58ms/step - loss: 173.3008 - mae: 173.3008 - val_loss: 591.1317 - val_mae: 591.1317\n",
            "Epoch 14/100\n",
            "22/22 [==============================] - 1s 33ms/step - loss: 149.7509 - mae: 149.7509 - val_loss: 557.0607 - val_mae: 557.0607\n",
            "Epoch 15/100\n",
            "22/22 [==============================] - 1s 45ms/step - loss: 295.8418 - mae: 295.8418 - val_loss: 1238.4834 - val_mae: 1238.4834\n",
            "Epoch 16/100\n",
            "22/22 [==============================] - 1s 61ms/step - loss: 883.9547 - mae: 883.9547 - val_loss: 1582.9958 - val_mae: 1582.9958\n",
            "Epoch 17/100\n",
            "22/22 [==============================] - 1s 37ms/step - loss: 251.7808 - mae: 251.7808 - val_loss: 883.9535 - val_mae: 883.9535\n",
            "Epoch 18/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 335.6385 - mae: 335.6385 - val_loss: 848.7709 - val_mae: 848.7709\n",
            "Epoch 19/100\n",
            "22/22 [==============================] - 1s 47ms/step - loss: 965.4781 - mae: 965.4781 - val_loss: 1750.0413 - val_mae: 1750.0413\n",
            "Epoch 20/100\n",
            "22/22 [==============================] - 1s 55ms/step - loss: 3366.5632 - mae: 3366.5632 - val_loss: 11051.0566 - val_mae: 11051.0566\n",
            "Epoch 21/100\n",
            "22/22 [==============================] - 1s 38ms/step - loss: 725.8094 - mae: 725.8094 - val_loss: 803.5903 - val_mae: 803.5903\n",
            "Epoch 22/100\n",
            "22/22 [==============================] - 1s 51ms/step - loss: 2170.3750 - mae: 2170.3750 - val_loss: 6150.3525 - val_mae: 6150.3525\n",
            "Epoch 23/100\n",
            "22/22 [==============================] - 1s 64ms/step - loss: 1114.7931 - mae: 1114.7931 - val_loss: 2292.0181 - val_mae: 2292.0181\n",
            "Epoch 24/100\n",
            "22/22 [==============================] - 1s 42ms/step - loss: 1483.7056 - mae: 1483.7056 - val_loss: 3444.0906 - val_mae: 3444.0906\n",
            "Epoch 25/100\n",
            "22/22 [==============================] - 1s 53ms/step - loss: 1319.6090 - mae: 1319.6090 - val_loss: 3649.5249 - val_mae: 3649.5249\n",
            "Epoch 26/100\n",
            "22/22 [==============================] - 1s 44ms/step - loss: 1046.3690 - mae: 1046.3690 - val_loss: 1795.2875 - val_mae: 1795.2875\n",
            "Epoch 27/100\n",
            "22/22 [==============================] - 1s 55ms/step - loss: 1382.2455 - mae: 1382.2455 - val_loss: 4193.4873 - val_mae: 4193.4873\n",
            "Epoch 28/100\n",
            "22/22 [==============================] - 1s 57ms/step - loss: 816.5045 - mae: 816.5045 - val_loss: 1193.1556 - val_mae: 1193.1556\n",
            "Epoch 29/100\n",
            "22/22 [==============================] - 1s 48ms/step - loss: 1232.2495 - mae: 1232.2495 - val_loss: 3528.8032 - val_mae: 3528.8032\n",
            "Epoch 30/100\n",
            "22/22 [==============================] - 1s 58ms/step - loss: 878.2924 - mae: 878.2924 - val_loss: 1523.6123 - val_mae: 1523.6123\n",
            "Epoch 31/100\n",
            "22/22 [==============================] - 1s 44ms/step - loss: 1137.5693 - mae: 1137.5693 - val_loss: 3225.4875 - val_mae: 3225.4875\n",
            "Epoch 32/100\n",
            "22/22 [==============================] - 1s 41ms/step - loss: 859.1855 - mae: 859.1855 - val_loss: 1548.5195 - val_mae: 1548.5195\n",
            "Epoch 33/100\n",
            "22/22 [==============================] - 1s 58ms/step - loss: 1070.3224 - mae: 1070.3224 - val_loss: 3011.2715 - val_mae: 3011.2715\n",
            "Epoch 34/100\n",
            "22/22 [==============================] - 1s 48ms/step - loss: 840.1406 - mae: 840.1406 - val_loss: 1567.1018 - val_mae: 1567.1018\n",
            "Epoch 35/100\n",
            "22/22 [==============================] - 1s 46ms/step - loss: 1000.2375 - mae: 1000.2375 - val_loss: 2765.0393 - val_mae: 2765.0393\n",
            "Epoch 36/100\n",
            "22/22 [==============================] - 1s 56ms/step - loss: 838.5800 - mae: 838.5800 - val_loss: 1639.9974 - val_mae: 1639.9974\n",
            "Epoch 37/100\n",
            "22/22 [==============================] - 1s 56ms/step - loss: 940.0231 - mae: 940.0231 - val_loss: 2575.4097 - val_mae: 2575.4097\n",
            "Epoch 38/100\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 828.6985 - mae: 828.6985 - val_loss: 1667.3926 - val_mae: 1667.3926\n",
            "Epoch 39/100\n",
            "22/22 [==============================] - 1s 37ms/step - loss: 891.5873 - mae: 891.5873 - val_loss: 2422.1387 - val_mae: 2422.1387\n",
            "Epoch 40/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 821.8396 - mae: 821.8396 - val_loss: 1704.1090 - val_mae: 1704.1090\n",
            "Epoch 41/100\n",
            "22/22 [==============================] - 1s 55ms/step - loss: 848.2068 - mae: 848.2068 - val_loss: 2299.9241 - val_mae: 2299.9241\n",
            "Epoch 42/100\n",
            "22/22 [==============================] - 1s 45ms/step - loss: 801.6447 - mae: 801.6447 - val_loss: 1672.2057 - val_mae: 1672.2057\n",
            "Epoch 43/100\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 823.4948 - mae: 823.4948 - val_loss: 2244.6196 - val_mae: 2244.6196\n",
            "Epoch 44/100\n",
            "22/22 [==============================] - 1s 38ms/step - loss: 781.9151 - mae: 781.9151 - val_loss: 1656.2594 - val_mae: 1656.2594\n",
            "Epoch 45/100\n",
            "22/22 [==============================] - 1s 68ms/step - loss: 772.7912 - mae: 772.7912 - val_loss: 2068.7817 - val_mae: 2068.7817\n",
            "Epoch 46/100\n",
            "22/22 [==============================] - 1s 65ms/step - loss: 779.3227 - mae: 779.3227 - val_loss: 1681.3033 - val_mae: 1681.3033\n",
            "Epoch 47/100\n",
            "22/22 [==============================] - 2s 91ms/step - loss: 745.4168 - mae: 745.4168 - val_loss: 2005.0336 - val_mae: 2005.0336\n",
            "Epoch 48/100\n",
            "22/22 [==============================] - 2s 111ms/step - loss: 757.9503 - mae: 757.9503 - val_loss: 1636.5117 - val_mae: 1636.5117\n",
            "Epoch 49/100\n",
            "22/22 [==============================] - 1s 59ms/step - loss: 720.7484 - mae: 720.7484 - val_loss: 1937.8057 - val_mae: 1937.8057\n",
            "Epoch 50/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 745.9089 - mae: 745.9089 - val_loss: 1633.2083 - val_mae: 1633.2083\n",
            "Epoch 51/100\n",
            "22/22 [==============================] - 1s 37ms/step - loss: 687.2789 - mae: 687.2789 - val_loss: 1835.9187 - val_mae: 1835.9187\n",
            "Epoch 52/100\n",
            "22/22 [==============================] - 1s 37ms/step - loss: 733.5498 - mae: 733.5498 - val_loss: 1609.5533 - val_mae: 1609.5533\n",
            "Epoch 53/100\n",
            "22/22 [==============================] - 1s 37ms/step - loss: 675.5009 - mae: 675.5009 - val_loss: 1834.6611 - val_mae: 1834.6611\n",
            "Epoch 54/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 697.8586 - mae: 697.8586 - val_loss: 1515.0764 - val_mae: 1515.0764\n",
            "Epoch 55/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 645.5388 - mae: 645.5388 - val_loss: 1726.4332 - val_mae: 1726.4332\n",
            "Epoch 56/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 692.0596 - mae: 692.0596 - val_loss: 1502.7385 - val_mae: 1502.7385\n",
            "Epoch 57/100\n",
            "22/22 [==============================] - 1s 57ms/step - loss: 638.1846 - mae: 638.1846 - val_loss: 1714.7576 - val_mae: 1714.7576\n",
            "Epoch 58/100\n",
            "22/22 [==============================] - 1s 52ms/step - loss: 684.2156 - mae: 684.2156 - val_loss: 1499.7180 - val_mae: 1499.7180\n",
            "Epoch 59/100\n",
            "22/22 [==============================] - 1s 34ms/step - loss: 624.3889 - mae: 624.3889 - val_loss: 1682.7279 - val_mae: 1682.7279\n",
            "Epoch 60/100\n",
            "22/22 [==============================] - 1s 37ms/step - loss: 672.0070 - mae: 672.0070 - val_loss: 1473.2826 - val_mae: 1473.2826\n",
            "Epoch 61/100\n",
            "22/22 [==============================] - 1s 48ms/step - loss: 609.5233 - mae: 609.5233 - val_loss: 1637.8141 - val_mae: 1637.8141\n",
            "Epoch 62/100\n",
            "22/22 [==============================] - 1s 33ms/step - loss: 664.9517 - mae: 664.9517 - val_loss: 1454.3899 - val_mae: 1454.3899\n",
            "Epoch 63/100\n",
            "22/22 [==============================] - 1s 34ms/step - loss: 606.4296 - mae: 606.4296 - val_loss: 1648.8645 - val_mae: 1648.8645\n",
            "Epoch 64/100\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 625.8018 - mae: 625.8018 - val_loss: 1348.4244 - val_mae: 1348.4244\n",
            "Epoch 65/100\n",
            "22/22 [==============================] - 1s 37ms/step - loss: 584.4439 - mae: 584.4439 - val_loss: 1573.6465 - val_mae: 1573.6465\n",
            "Epoch 66/100\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 621.9789 - mae: 621.9789 - val_loss: 1337.5438 - val_mae: 1337.5438\n",
            "Epoch 67/100\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 581.8403 - mae: 581.8403 - val_loss: 1591.0139 - val_mae: 1591.0139\n",
            "Epoch 68/100\n",
            "22/22 [==============================] - 2s 102ms/step - loss: 607.7192 - mae: 607.7192 - val_loss: 1318.7130 - val_mae: 1318.7130\n",
            "Epoch 69/100\n",
            "22/22 [==============================] - 1s 53ms/step - loss: 562.3650 - mae: 562.3650 - val_loss: 1529.2673 - val_mae: 1529.2673\n",
            "Epoch 70/100\n",
            "22/22 [==============================] - 1s 57ms/step - loss: 589.7974 - mae: 589.7974 - val_loss: 1269.7356 - val_mae: 1269.7356\n",
            "Epoch 71/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 553.8627 - mae: 553.8627 - val_loss: 1559.3728 - val_mae: 1559.3728\n",
            "Epoch 72/100\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 547.3618 - mae: 547.3618 - val_loss: 1157.9409 - val_mae: 1157.9409\n",
            "Epoch 73/100\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 522.2401 - mae: 522.2401 - val_loss: 1385.4376 - val_mae: 1385.4376\n",
            "Epoch 74/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 549.6708 - mae: 549.6708 - val_loss: 1119.5627 - val_mae: 1119.5627\n",
            "Epoch 75/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 543.8048 - mae: 543.8048 - val_loss: 1449.7334 - val_mae: 1449.7334\n",
            "Epoch 76/100\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 557.1804 - mae: 557.1804 - val_loss: 1185.1390 - val_mae: 1185.1390\n",
            "Epoch 77/100\n",
            "22/22 [==============================] - 1s 59ms/step - loss: 534.8544 - mae: 534.8544 - val_loss: 1449.5079 - val_mae: 1449.5079\n",
            "Epoch 78/100\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 558.0745 - mae: 558.0745 - val_loss: 1164.7793 - val_mae: 1164.7793\n",
            "Epoch 79/100\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 539.9617 - mae: 539.9617 - val_loss: 1467.1324 - val_mae: 1467.1324\n",
            "Epoch 80/100\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 558.5013 - mae: 558.5013 - val_loss: 1200.6499 - val_mae: 1200.6499\n",
            "Epoch 81/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 535.0538 - mae: 535.0538 - val_loss: 1485.2058 - val_mae: 1485.2058\n",
            "Epoch 82/100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 534.5525 - mae: 534.5525 - val_loss: 1125.7502 - val_mae: 1125.7502\n",
            "Epoch 83/100\n",
            "22/22 [==============================] - 1s 37ms/step - loss: 501.2899 - mae: 501.2899 - val_loss: 1372.4803 - val_mae: 1372.4803\n",
            "Epoch 84/100\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 512.9403 - mae: 512.9403 - val_loss: 929.5073 - val_mae: 929.5073\n",
            "Epoch 85/100\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 597.6368 - mae: 597.6368 - val_loss: 1170.1973 - val_mae: 1170.1973\n",
            "Epoch 86/100\n",
            "22/22 [==============================] - 1s 54ms/step - loss: 599.8586 - mae: 599.8586 - val_loss: 1240.6841 - val_mae: 1240.6841\n",
            "Epoch 87/100\n",
            "22/22 [==============================] - 1s 53ms/step - loss: 581.5392 - mae: 581.5392 - val_loss: 1699.6926 - val_mae: 1699.6926\n",
            "Epoch 88/100\n",
            "22/22 [==============================] - 1s 45ms/step - loss: 547.1285 - mae: 547.1285 - val_loss: 1225.4479 - val_mae: 1225.4479\n",
            "Epoch 89/100\n",
            "22/22 [==============================] - 1s 54ms/step - loss: 507.6323 - mae: 507.6323 - val_loss: 1463.7708 - val_mae: 1463.7708\n",
            "Epoch 90/100\n",
            "22/22 [==============================] - 1s 52ms/step - loss: 489.6356 - mae: 489.6356 - val_loss: 1009.7062 - val_mae: 1009.7062\n",
            "Epoch 91/100\n",
            "22/22 [==============================] - 1s 52ms/step - loss: 512.3692 - mae: 512.3692 - val_loss: 1246.9954 - val_mae: 1246.9954\n",
            "Epoch 92/100\n",
            "22/22 [==============================] - 1s 59ms/step - loss: 553.9135 - mae: 553.9135 - val_loss: 1096.9299 - val_mae: 1096.9299\n",
            "Epoch 93/100\n",
            "22/22 [==============================] - 1s 51ms/step - loss: 564.6238 - mae: 564.6238 - val_loss: 1635.4812 - val_mae: 1635.4812\n",
            "Epoch 94/100\n",
            "22/22 [==============================] - 1s 42ms/step - loss: 540.8781 - mae: 540.8781 - val_loss: 1217.5369 - val_mae: 1217.5369\n",
            "Epoch 95/100\n",
            "22/22 [==============================] - 1s 53ms/step - loss: 502.6074 - mae: 502.6074 - val_loss: 1458.9377 - val_mae: 1458.9377\n",
            "Epoch 96/100\n",
            "22/22 [==============================] - 2s 96ms/step - loss: 489.6848 - mae: 489.6848 - val_loss: 1081.6904 - val_mae: 1081.6904\n",
            "Epoch 97/100\n",
            "22/22 [==============================] - 1s 45ms/step - loss: 472.0957 - mae: 472.0957 - val_loss: 1312.9199 - val_mae: 1312.9199\n",
            "Epoch 98/100\n",
            "22/22 [==============================] - 1s 51ms/step - loss: 498.0388 - mae: 498.0388 - val_loss: 1054.1975 - val_mae: 1054.1975\n",
            "Epoch 99/100\n",
            "22/22 [==============================] - 1s 52ms/step - loss: 498.2549 - mae: 498.2549 - val_loss: 1371.8600 - val_mae: 1371.8600\n",
            "Epoch 100/100\n",
            "22/22 [==============================] - 1s 65ms/step - loss: 504.8987 - mae: 504.8987 - val_loss: 1015.9725 - val_mae: 1015.9725\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc71ef713f0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the test set\n",
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TONY5Lw4Fks",
        "outputId": "04fbd49a-4d1a-48f3-84a8-ee597df4abdb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 17ms/step - loss: 1015.9725 - mae: 1015.9725\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1015.9724731445312, 1015.9724731445312]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Multivariate modelling with 3 features"
      ],
      "metadata": {
        "id": "lMmErrOQ5cUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining the prices, block reward size and days of the week"
      ],
      "metadata": {
        "id": "0KIowNOz8QHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "uVjhdx3y5nXH",
        "outputId": "18ab38f8-a9f2-4353-f6a0-532791ac13e7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                Open      High       Low     Close        Volume    Market Cap\n",
              "Date                                                                          \n",
              "2013-09-30    132.68    134.63    131.55    132.18  0.000000e+00  1.567875e+09\n",
              "2013-10-01    132.05    133.59    102.25    114.13  0.000000e+00  1.501799e+09\n",
              "2013-10-02    114.45    123.63    111.82    123.63  0.000000e+00  1.412675e+09\n",
              "2013-10-03    123.41    130.09    123.41    129.01  0.000000e+00  1.500255e+09\n",
              "2013-10-04    128.63    130.44    128.03    128.55  0.000000e+00  1.522529e+09\n",
              "...              ...       ...       ...       ...           ...           ...\n",
              "2023-06-08  26509.38  26766.28  26325.91  26489.46  2.879870e+10  5.143227e+11\n",
              "2023-06-09  26481.47  26500.50  25491.91  25842.84  3.586479e+10  5.009359e+11\n",
              "2023-06-10  25820.23  26147.84  25676.59  25920.86  3.299485e+10  5.011299e+11\n",
              "2023-06-11  25922.89  26081.61  25662.95  25887.59  3.018877e+10  5.018320e+11\n",
              "2023-06-12  25910.04  26300.08  25760.04  25910.03  3.377094e+10  5.042019e+11\n",
              "\n",
              "[3543 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2206f70-80e7-46b4-aa3b-e42b8c5dd8f6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Market Cap</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-09-30</th>\n",
              "      <td>132.68</td>\n",
              "      <td>134.63</td>\n",
              "      <td>131.55</td>\n",
              "      <td>132.18</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.567875e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-01</th>\n",
              "      <td>132.05</td>\n",
              "      <td>133.59</td>\n",
              "      <td>102.25</td>\n",
              "      <td>114.13</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.501799e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-02</th>\n",
              "      <td>114.45</td>\n",
              "      <td>123.63</td>\n",
              "      <td>111.82</td>\n",
              "      <td>123.63</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.412675e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-03</th>\n",
              "      <td>123.41</td>\n",
              "      <td>130.09</td>\n",
              "      <td>123.41</td>\n",
              "      <td>129.01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.500255e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-04</th>\n",
              "      <td>128.63</td>\n",
              "      <td>130.44</td>\n",
              "      <td>128.03</td>\n",
              "      <td>128.55</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.522529e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-06-08</th>\n",
              "      <td>26509.38</td>\n",
              "      <td>26766.28</td>\n",
              "      <td>26325.91</td>\n",
              "      <td>26489.46</td>\n",
              "      <td>2.879870e+10</td>\n",
              "      <td>5.143227e+11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-06-09</th>\n",
              "      <td>26481.47</td>\n",
              "      <td>26500.50</td>\n",
              "      <td>25491.91</td>\n",
              "      <td>25842.84</td>\n",
              "      <td>3.586479e+10</td>\n",
              "      <td>5.009359e+11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-06-10</th>\n",
              "      <td>25820.23</td>\n",
              "      <td>26147.84</td>\n",
              "      <td>25676.59</td>\n",
              "      <td>25920.86</td>\n",
              "      <td>3.299485e+10</td>\n",
              "      <td>5.011299e+11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-06-11</th>\n",
              "      <td>25922.89</td>\n",
              "      <td>26081.61</td>\n",
              "      <td>25662.95</td>\n",
              "      <td>25887.59</td>\n",
              "      <td>3.018877e+10</td>\n",
              "      <td>5.018320e+11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-06-12</th>\n",
              "      <td>25910.04</td>\n",
              "      <td>26300.08</td>\n",
              "      <td>25760.04</td>\n",
              "      <td>25910.03</td>\n",
              "      <td>3.377094e+10</td>\n",
              "      <td>5.042019e+11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3543 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2206f70-80e7-46b4-aa3b-e42b8c5dd8f6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2206f70-80e7-46b4-aa3b-e42b8c5dd8f6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2206f70-80e7-46b4-aa3b-e42b8c5dd8f6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the day of the week to the dataframe (could influence price)\n",
        "# e.g. monday = 0, tuesday = 1 etc.\n",
        "import datetime\n",
        "# Creating a day of week feature\n",
        "df['day_of_week'] = df.index.dayofweek\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "rNNVAK4l6pQs",
        "outputId": "8025d9f8-1dc1-4485-a6e3-408f39c05825"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Open    High     Low   Close  Volume    Market Cap  day_of_week\n",
              "Date                                                                         \n",
              "2013-09-30  132.68  134.63  131.55  132.18     0.0  1.567875e+09            0\n",
              "2013-10-01  132.05  133.59  102.25  114.13     0.0  1.501799e+09            1\n",
              "2013-10-02  114.45  123.63  111.82  123.63     0.0  1.412675e+09            2\n",
              "2013-10-03  123.41  130.09  123.41  129.01     0.0  1.500255e+09            3\n",
              "2013-10-04  128.63  130.44  128.03  128.55     0.0  1.522529e+09            4\n",
              "2013-10-05  128.36  129.66  126.15  129.00     0.0  1.511180e+09            5\n",
              "2013-10-06  129.43  130.27  126.36  126.94     0.0  1.518677e+09            6\n",
              "2013-10-07  126.74  127.47  124.71  126.00     0.0  1.487193e+09            0\n",
              "2013-10-08  125.85  131.75  125.58  130.69     0.0  1.524165e+09            1\n",
              "2013-10-09  130.67  131.50  129.26  130.59     0.0  1.542156e+09            2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ca232e5-9a97-4623-af98-67e5008e5c2d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Market Cap</th>\n",
              "      <th>day_of_week</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-09-30</th>\n",
              "      <td>132.68</td>\n",
              "      <td>134.63</td>\n",
              "      <td>131.55</td>\n",
              "      <td>132.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.567875e+09</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-01</th>\n",
              "      <td>132.05</td>\n",
              "      <td>133.59</td>\n",
              "      <td>102.25</td>\n",
              "      <td>114.13</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.501799e+09</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-02</th>\n",
              "      <td>114.45</td>\n",
              "      <td>123.63</td>\n",
              "      <td>111.82</td>\n",
              "      <td>123.63</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.412675e+09</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-03</th>\n",
              "      <td>123.41</td>\n",
              "      <td>130.09</td>\n",
              "      <td>123.41</td>\n",
              "      <td>129.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.500255e+09</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-04</th>\n",
              "      <td>128.63</td>\n",
              "      <td>130.44</td>\n",
              "      <td>128.03</td>\n",
              "      <td>128.55</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.522529e+09</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-05</th>\n",
              "      <td>128.36</td>\n",
              "      <td>129.66</td>\n",
              "      <td>126.15</td>\n",
              "      <td>129.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.511180e+09</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-06</th>\n",
              "      <td>129.43</td>\n",
              "      <td>130.27</td>\n",
              "      <td>126.36</td>\n",
              "      <td>126.94</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.518677e+09</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-07</th>\n",
              "      <td>126.74</td>\n",
              "      <td>127.47</td>\n",
              "      <td>124.71</td>\n",
              "      <td>126.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.487193e+09</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-08</th>\n",
              "      <td>125.85</td>\n",
              "      <td>131.75</td>\n",
              "      <td>125.58</td>\n",
              "      <td>130.69</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.524165e+09</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-10-09</th>\n",
              "      <td>130.67</td>\n",
              "      <td>131.50</td>\n",
              "      <td>129.26</td>\n",
              "      <td>130.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.542156e+09</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ca232e5-9a97-4623-af98-67e5008e5c2d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0ca232e5-9a97-4623-af98-67e5008e5c2d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0ca232e5-9a97-4623-af98-67e5008e5c2d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hyper parameters\n",
        "HORIZON = 1\n",
        "WINDOW_SIZE = 7\n",
        "\n",
        "bitcoin_prices_windowed['day_of_week'] = bitcoin_prices_windowed.index.dayofweek"
      ],
      "metadata": {
        "id": "AnAQOsCe6wuf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting three kinds of data (univariate , multivariate and the day of week)\n",
        "\n",
        "# Univariate data\n",
        "full_windows , full_labels = make_windows_scaled(prices)\n",
        "train_windows , test_windows , train_labels , test_labels = make_train_test_splits(full_windows , full_labels)\n",
        "\n",
        "# Multivaritate dat\n",
        "X = bitcoin_prices_windowed.dropna().drop('Price' , axis = 1).astype(np.float32)\n",
        "y = bitcoin_prices_windowed.dropna()['Price'].astype(np.float32)\n",
        "\n",
        "# Day of week\n",
        "day_of_week = bitcoin_prices_windowed.dropna()['day_of_week'].to_list()"
      ],
      "metadata": {
        "id": "vVFKvQQ361De"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the shapes\n",
        "print(full_windows.shape , full_labels.shape)\n",
        "print(X.shape , y.shape)\n",
        "print(len(day_of_week))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNqiwcYO774s",
        "outputId": "72ed2d28-751c-4c1d-f0b4-08e60d5c2554"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3536, 7) (3536, 1)\n",
            "(3536, 9) (3536,)\n",
            "3536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the datasets"
      ],
      "metadata": {
        "id": "djhaCa_f8e3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the multivariate and the day_of_week to train and test splits\n",
        "split_size = int(len(X) * 0.8)\n",
        "train_block_rewards , test_block_rewards = X[:split_size] , X[split_size:]\n",
        "train_days , test_days = day_of_week[:split_size] , day_of_week[split_size:]\n",
        "\n",
        "len(train_block_rewards), len(train_days) , len(test_block_rewards) , len(test_days)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw88XtxY8MN5",
        "outputId": "cf495f4d-fdd5-4009-8f79-c48d801751b5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2828, 2828, 708, 708)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a performant dataset for train and test\n",
        "\n",
        "train_data_tribid = tf.data.Dataset.from_tensor_slices((train_windows ,\n",
        "                                                        train_block_rewards ,\n",
        "                                                        train_days))\n",
        "\n",
        "train_labels_tribid = tf.data.Dataset.from_tensor_slices(train_labels)\n",
        "\n",
        "# The test/val split\n",
        "test_data_tribid = tf.data.Dataset.from_tensor_slices((test_windows ,\n",
        "                                                       test_block_rewards ,\n",
        "                                                       test_days))\n",
        "\n",
        "test_labels_tribid = tf.data.Dataset.from_tensor_slices(test_labels)\n",
        "\n",
        "# Zipping the data and labels into one complete dataset\n",
        "tribid_train_ds = tf.data.Dataset.zip((train_data_tribid , train_labels_tribid))\n",
        "tribid_test_ds = tf.data.Dataset.zip((test_data_tribid , test_labels_tribid))\n",
        "\n",
        "# Applying prefetch and batching the dataset\n",
        "tribid_train_ds = tribid_train_ds.batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "tribid_test_ds = tribid_test_ds.batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "tribid_train_ds ,tribid_test_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwuWkCcB8XhE",
        "outputId": "0bac8d56-32c4-4b73-a7d7-c269f99576ab"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<_PrefetchDataset element_spec=((TensorSpec(shape=(None, 7), dtype=tf.float64, name=None), TensorSpec(shape=(None, 9), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None)), TensorSpec(shape=(None, 1), dtype=tf.float64, name=None))>,\n",
              " <_PrefetchDataset element_spec=((TensorSpec(shape=(None, 7), dtype=tf.float64, name=None), TensorSpec(shape=(None, 9), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None)), TensorSpec(shape=(None, 1), dtype=tf.float64, name=None))>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the model"
      ],
      "metadata": {
        "id": "32NpaxC78gg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a tribid model\n",
        "\n",
        "input_windows = layers.Input(shape = (7,) , dtype=tf.float64 , name='Window Inputs')\n",
        "expand_layer_1 = layers.Lambda(lambda x: tf.expand_dims(x , axis = 1))(input_windows) # resizing\n",
        "conv1 = layers.Conv1D(filters= 32 , kernel_size=5 , padding='causal' , activation= 'relu')(expand_layer_1)\n",
        "window_model = tf.keras.Model(input_windows , conv1 , name = 'Windowed model')\n",
        "\n",
        "input_blocks = layers.Input(shape = (9,) , dtype= tf.float32 , name ='Block rewards input')\n",
        "expand_layer_2 = layers.Lambda(lambda x: tf.expand_dims(x , axis = 1))(input_blocks) # resizing\n",
        "conv2 = layers.Conv1D(filters = 32 , kernel_size= 5 , activation= 'relu' , padding = 'causal')(expand_layer_2)\n",
        "block_model = tf.keras.Model(input_blocks , conv2 , name = 'Block rewards model')\n",
        "\n",
        "\n",
        "# Use expand dims to match the same shape output (None , 1 , 128)\n",
        "# whereas without expand dims it would be (None , 128)\n",
        "input_days = layers.Input(shape= (1,) , dtype = tf.int32 , name ='Days of week Input')\n",
        "expand_layer_3 = layers.Lambda(lambda x: tf.expand_dims(x , axis = 1))(input_days)\n",
        "dense = layers.Dense(128 , activation= 'relu')(expand_layer_3)\n",
        "days_model = tf.keras.Model(input_days , dense , name = 'Days Model')\n",
        "\n",
        "# Concatenating the inputs\n",
        "concat = layers.Concatenate(name = 'combined_outputs' )([window_model.output ,\n",
        "                                                           block_model.output ,\n",
        "                                                           days_model.output])\n",
        "\n",
        "# Creating the output layer\n",
        "dropout = layers.Dropout(0.5)(concat)\n",
        "output_layer = layers.Dense(1 , activation = 'linear')(dropout)\n",
        "\n",
        "# Putting everything into a model\n",
        "tribrid_model = tf.keras.Model(inputs = [window_model.input ,\n",
        "                                        block_model.input ,\n",
        "                                        days_model.input] ,\n",
        "                              outputs = output_layer)\n",
        "tribrid_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q4PZo908i1m",
        "outputId": "323f33fb-1e70-4c1b-d397-3964ab0aaf5b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Window Inputs (InputLayer)     [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " Block rewards input (InputLaye  [(None, 9)]         0           []                               \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " Days of week Input (InputLayer  [(None, 1)]         0           []                               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1, 7)         0           ['Window Inputs[0][0]']          \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 1, 9)         0           ['Block rewards input[0][0]']    \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 1, 1)         0           ['Days of week Input[0][0]']     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 1, 32)        1152        ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 1, 32)        1472        ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " dense_26 (Dense)               (None, 1, 128)       256         ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " combined_outputs (Concatenate)  (None, 1, 192)      0           ['conv1d[0][0]',                 \n",
            "                                                                  'conv1d_1[0][0]',               \n",
            "                                                                  'dense_26[0][0]']               \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 1, 192)       0           ['combined_outputs[0][0]']       \n",
            "                                                                                                  \n",
            " dense_27 (Dense)               (None, 1, 1)         193         ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,073\n",
            "Trainable params: 3,073\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the token, char, positional embedding model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(tribrid_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "AVX8AkUc-MyP",
        "outputId": "aa9fa5bb-bcc4-4ea7-b402-54e319fe186e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling and fitting the model\n",
        "tribrid_model.compile(loss='mae',\n",
        "                     optimizer=tf.keras.optimizers.Adam(),\n",
        "                     metrics=['mae'])\n",
        "\n",
        "# Fitting the model\n",
        "tribrid_model.fit(tribid_train_ds,\n",
        "                 epochs = 20,\n",
        "                 validation_data = tribid_test_ds , verbose = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoxTQuss-ReT",
        "outputId": "f3b63f89-6585-41f4-b45f-8ce57bb567fc"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "23/23 - 8s - loss: 790.6692 - mae: 790.6692 - val_loss: 626.2709 - val_mae: 626.2709 - 8s/epoch - 356ms/step\n",
            "Epoch 2/20\n",
            "23/23 - 0s - loss: 624.4527 - mae: 624.4527 - val_loss: 49.6318 - val_mae: 49.6318 - 116ms/epoch - 5ms/step\n",
            "Epoch 3/20\n",
            "23/23 - 0s - loss: 507.0270 - mae: 507.0270 - val_loss: 153.2520 - val_mae: 153.2520 - 129ms/epoch - 6ms/step\n",
            "Epoch 4/20\n",
            "23/23 - 0s - loss: 352.7021 - mae: 352.7021 - val_loss: 38.3141 - val_mae: 38.3141 - 114ms/epoch - 5ms/step\n",
            "Epoch 5/20\n",
            "23/23 - 0s - loss: 286.1974 - mae: 286.1974 - val_loss: 88.1203 - val_mae: 88.1203 - 124ms/epoch - 5ms/step\n",
            "Epoch 6/20\n",
            "23/23 - 0s - loss: 231.0936 - mae: 231.0936 - val_loss: 43.9808 - val_mae: 43.9808 - 117ms/epoch - 5ms/step\n",
            "Epoch 7/20\n",
            "23/23 - 0s - loss: 188.1852 - mae: 188.1852 - val_loss: 183.2830 - val_mae: 183.2830 - 118ms/epoch - 5ms/step\n",
            "Epoch 8/20\n",
            "23/23 - 0s - loss: 133.1009 - mae: 133.1009 - val_loss: 317.3421 - val_mae: 317.3421 - 130ms/epoch - 6ms/step\n",
            "Epoch 9/20\n",
            "23/23 - 0s - loss: 75.3673 - mae: 75.3673 - val_loss: 32.4086 - val_mae: 32.4086 - 118ms/epoch - 5ms/step\n",
            "Epoch 10/20\n",
            "23/23 - 0s - loss: 37.8890 - mae: 37.8890 - val_loss: 19.4450 - val_mae: 19.4450 - 142ms/epoch - 6ms/step\n",
            "Epoch 11/20\n",
            "23/23 - 0s - loss: 12.5415 - mae: 12.5415 - val_loss: 10.6717 - val_mae: 10.6717 - 126ms/epoch - 5ms/step\n",
            "Epoch 12/20\n",
            "23/23 - 0s - loss: 1.2417 - mae: 1.2417 - val_loss: 1.0225 - val_mae: 1.0225 - 120ms/epoch - 5ms/step\n",
            "Epoch 13/20\n",
            "23/23 - 0s - loss: 0.5623 - mae: 0.5623 - val_loss: 0.7618 - val_mae: 0.7618 - 119ms/epoch - 5ms/step\n",
            "Epoch 14/20\n",
            "23/23 - 0s - loss: 0.4464 - mae: 0.4464 - val_loss: 0.7578 - val_mae: 0.7578 - 125ms/epoch - 5ms/step\n",
            "Epoch 15/20\n",
            "23/23 - 0s - loss: 0.5856 - mae: 0.5856 - val_loss: 0.6105 - val_mae: 0.6105 - 122ms/epoch - 5ms/step\n",
            "Epoch 16/20\n",
            "23/23 - 0s - loss: 0.3496 - mae: 0.3496 - val_loss: 0.2467 - val_mae: 0.2467 - 130ms/epoch - 6ms/step\n",
            "Epoch 17/20\n",
            "23/23 - 0s - loss: 0.2750 - mae: 0.2750 - val_loss: 0.2052 - val_mae: 0.2052 - 121ms/epoch - 5ms/step\n",
            "Epoch 18/20\n",
            "23/23 - 0s - loss: 0.2790 - mae: 0.2790 - val_loss: 0.3327 - val_mae: 0.3327 - 128ms/epoch - 6ms/step\n",
            "Epoch 19/20\n",
            "23/23 - 0s - loss: 0.2446 - mae: 0.2446 - val_loss: 0.1185 - val_mae: 0.1185 - 117ms/epoch - 5ms/step\n",
            "Epoch 20/20\n",
            "23/23 - 0s - loss: 0.2477 - mae: 0.2477 - val_loss: 0.1449 - val_mae: 0.1449 - 113ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc720147700>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "tribrid_model.evaluate(tribid_test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb2pJzTQAKpl",
        "outputId": "cb450531-b952-493b-839f-94b5a59d6a4d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 4ms/step - loss: 0.1449 - mae: 0.1449\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.14486482739448547, 0.14486482739448547]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Make prediction intervals for future forecasts"
      ],
      "metadata": {
        "id": "yxfLuLiqAO_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Train ensemble model on whole data\n",
        "* Make one dataset (no train/test)\n",
        "* Make a function to take number of iteration and different loss functions to train the model with"
      ],
      "metadata": {
        "id": "AS8UlPCIAj7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make one whole dataset (with the updated bitcoin prices 2014 - 2021)\n",
        "\n",
        "X_all = bitcoin_prices_windowed.drop(['Price' , 'block_reward' , 'day_of_week'] , axis = 1).dropna().to_numpy()\n",
        "y_all = bitcoin_prices_windowed.dropna()['Price'].to_numpy()\n",
        "\n",
        "whole_ds = tf.data.Dataset.from_tensor_slices((X_all , y_all))\n",
        "whole_ds = whole_ds.batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "whole_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAPfOWgqAi5a",
        "outputId": "93e061ed-eed6-4a9a-f291-007557a3be9f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 7), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the function (same as in other workbook)\n",
        "\n",
        "def get_ensemble_models(horizon = HORIZON ,\n",
        "                        dataset = whole_ds ,\n",
        "                        num_iter = 10 ,\n",
        "                        num_epochs = 100 ,\n",
        "                        loss_fns = ['mae' , 'mse' , 'mape']):\n",
        "\n",
        "\n",
        "  # Make a empty list of the ensemble models\n",
        "  ensemble_models = []\n",
        "\n",
        "  # Create num_iter number of models per loss functions\n",
        "  for i in range(num_iter):\n",
        "    for loss_functions in loss_fns:\n",
        "      print(f'Optimizing model by reducing: {loss_functions} for {num_epochs} epochs, model number: {i}')\n",
        "\n",
        "      model = tf.keras.Sequential([\n",
        "          layers.Dense(128 , kernel_initializer='he_normal' , activation= 'relu'),\n",
        "          layers.Dense(128 , kernel_initializer= 'he_normal', activation= 'relu'),\n",
        "          layers.Dense(HORIZON)\n",
        "      ])\n",
        "\n",
        "      # Compiling the model\n",
        "      model.compile(loss = loss_functions ,\n",
        "                    optimizer = 'adam' , metrics = ['mae' , 'mse'])\n",
        "\n",
        "      # Fit the model\n",
        "      model.fit(dataset ,\n",
        "                epochs = num_epochs ,\n",
        "                verbose = 0,\n",
        "                callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"loss\",\n",
        "                                                            patience=200,\n",
        "                                                            restore_best_weights=True),\n",
        "                           tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\",\n",
        "                                                                patience=100,\n",
        "                                                                verbose=1)])\n",
        "\n",
        "      ensemble_models.append(model)\n",
        "\n",
        "  return ensemble_models"
      ],
      "metadata": {
        "id": "SVKW1InSBTUf"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the above function\n",
        "ensemble_models = get_ensemble_models(num_iter=5 , num_epochs= 1000 , horizon = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD2-3f4pBcJX",
        "outputId": "014dfcb2-0809-4536-86b6-003ea707b146"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizing model by reducing: mae for 1000 epochs, model number: 0\n",
            "\n",
            "Epoch 530: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Optimizing model by reducing: mse for 1000 epochs, model number: 0\n",
            "\n",
            "Epoch 426: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 581: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mape for 1000 epochs, model number: 0\n",
            "\n",
            "Epoch 307: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 407: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mae for 1000 epochs, model number: 1\n",
            "\n",
            "Epoch 420: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 674: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mse for 1000 epochs, model number: 1\n",
            "\n",
            "Epoch 393: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 494: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mape for 1000 epochs, model number: 1\n",
            "\n",
            "Epoch 288: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 388: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mae for 1000 epochs, model number: 2\n",
            "\n",
            "Epoch 669: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Optimizing model by reducing: mse for 1000 epochs, model number: 2\n",
            "\n",
            "Epoch 355: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 456: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 556: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Optimizing model by reducing: mape for 1000 epochs, model number: 2\n",
            "\n",
            "Epoch 401: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 501: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mae for 1000 epochs, model number: 3\n",
            "\n",
            "Epoch 414: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Optimizing model by reducing: mse for 1000 epochs, model number: 3\n",
            "\n",
            "Epoch 522: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 681: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mape for 1000 epochs, model number: 3\n",
            "\n",
            "Epoch 132: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 235: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 934: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Optimizing model by reducing: mae for 1000 epochs, model number: 4\n",
            "\n",
            "Epoch 449: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 653: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 853: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Optimizing model by reducing: mse for 1000 epochs, model number: 4\n",
            "\n",
            "Epoch 457: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 558: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Optimizing model by reducing: mape for 1000 epochs, model number: 4\n",
            "\n",
            "Epoch 297: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 397: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making future forecastts of Bitcoins (using the whole data) (same function as in other workbook)\n",
        "def make_future_forecast(values , model_list , into_future , window_size):\n",
        "\n",
        "  future_forecast = []\n",
        "  last_window = values[-window_size:]\n",
        "\n",
        "  for _ in range(into_future):\n",
        "    for model in model_list:\n",
        "\n",
        "      future_pred = model.predict(tf.expand_dims(last_window, axis= 0))\n",
        "      #future_pred = model.predict(last_window)\n",
        "      print(f'Predicing on: \\n {last_window} --> Prediction: {tf.squeeze(future_pred).numpy()}\\n')\n",
        "\n",
        "      future_forecast.append(tf.squeeze(future_pred).numpy())\n",
        "\n",
        "      # Update the last window\n",
        "      last_window = np.append(last_window , future_pred)[-window_size:]\n",
        "  return future_forecast"
      ],
      "metadata": {
        "id": "YcPkFkvzBgrS"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the future forecast\n",
        "future_forecast = make_future_forecast(y_all , ensemble_models , into_future= 14 , window_size = 7 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfHUc7-pCHwG",
        "outputId": "a81f7188-5646-4dd5-f6ef-b74370dfcd91"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 69ms/step\n",
            "Predicing on: \n",
            " [26359.59 26493.46 26489.46 25842.84 25920.86 25887.59 25910.03] --> Prediction: 26172.103515625\n",
            "\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "Predicing on: \n",
            " [26493.46       26489.46       25842.84       25920.86\n",
            " 25887.59       25910.03       26172.10351562] --> Prediction: 26538.130859375\n",
            "\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "Predicing on: \n",
            " [26489.46       25842.84       25920.86       25887.59\n",
            " 25910.03       26172.10351562 26538.13085938] --> Prediction: 26512.921875\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 27 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc73048f1c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 79ms/step\n",
            "Predicing on: \n",
            " [25842.84       25920.86       25887.59       25910.03\n",
            " 26172.10351562 26538.13085938 26512.921875  ] --> Prediction: 25917.232421875\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 28 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc73048c940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 72ms/step\n",
            "Predicing on: \n",
            " [25920.86       25887.59       25910.03       26172.10351562\n",
            " 26538.13085938 26512.921875   25917.23242188] --> Prediction: 25914.103515625\n",
            "\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "Predicing on: \n",
            " [25887.59       25910.03       26172.10351562 26538.13085938\n",
            " 26512.921875   25917.23242188 25914.10351562] --> Prediction: 25546.994140625\n",
            "\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "Predicing on: \n",
            " [25910.03       26172.10351562 26538.13085938 26512.921875\n",
            " 25917.23242188 25914.10351562 25546.99414062] --> Prediction: 25738.900390625\n",
            "\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "Predicing on: \n",
            " [26172.10351562 26538.13085938 26512.921875   25917.23242188\n",
            " 25914.10351562 25546.99414062 25738.90039062] --> Prediction: 26182.140625\n",
            "\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "Predicing on: \n",
            " [26538.13085938 26512.921875   25917.23242188 25914.10351562\n",
            " 25546.99414062 25738.90039062 26182.140625  ] --> Prediction: 26513.94140625\n",
            "\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "Predicing on: \n",
            " [26512.921875   25917.23242188 25914.10351562 25546.99414062\n",
            " 25738.90039062 26182.140625   26513.94140625] --> Prediction: 26270.775390625\n",
            "\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "Predicing on: \n",
            " [25917.23242188 25914.10351562 25546.99414062 25738.90039062\n",
            " 26182.140625   26513.94140625 26270.77539062] --> Prediction: 25912.0390625\n",
            "\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "Predicing on: \n",
            " [25914.10351562 25546.99414062 25738.90039062 26182.140625\n",
            " 26513.94140625 26270.77539062 25912.0390625 ] --> Prediction: 25882.8984375\n",
            "\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "Predicing on: \n",
            " [25546.99414062 25738.90039062 26182.140625   26513.94140625\n",
            " 26270.77539062 25912.0390625  25882.8984375 ] --> Prediction: 25586.263671875\n",
            "\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "Predicing on: \n",
            " [25738.90039062 26182.140625   26513.94140625 26270.77539062\n",
            " 25912.0390625  25882.8984375  25586.26367188] --> Prediction: 25649.521484375\n",
            "\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "Predicing on: \n",
            " [26182.140625   26513.94140625 26270.77539062 25912.0390625\n",
            " 25882.8984375  25586.26367188 25649.52148438] --> Prediction: 26152.513671875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [26513.94140625 26270.77539062 25912.0390625  25882.8984375\n",
            " 25586.26367188 25649.52148438 26152.51367188] --> Prediction: 26344.658203125\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [26270.77539062 25912.0390625  25882.8984375  25586.26367188\n",
            " 25649.52148438 26152.51367188 26344.65820312] --> Prediction: 26241.2265625\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25912.0390625  25882.8984375  25586.26367188 25649.52148438\n",
            " 26152.51367188 26344.65820312 26241.2265625 ] --> Prediction: 25967.037109375\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25882.8984375  25586.26367188 25649.52148438 26152.51367188\n",
            " 26344.65820312 26241.2265625  25967.03710938] --> Prediction: 25915.982421875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25586.26367188 25649.52148438 26152.51367188 26344.65820312\n",
            " 26241.2265625  25967.03710938 25915.98242188] --> Prediction: 25542.951171875\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicing on: \n",
            " [25649.52148438 26152.51367188 26344.65820312 26241.2265625\n",
            " 25967.03710938 25915.98242188 25542.95117188] --> Prediction: 25332.845703125\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicing on: \n",
            " [26152.51367188 26344.65820312 26241.2265625  25967.03710938\n",
            " 25915.98242188 25542.95117188 25332.84570312] --> Prediction: 25988.923828125\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [26344.65820312 26241.2265625  25967.03710938 25915.98242188\n",
            " 25542.95117188 25332.84570312 25988.92382812] --> Prediction: 26324.697265625\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [26241.2265625  25967.03710938 25915.98242188 25542.95117188\n",
            " 25332.84570312 25988.92382812 26324.69726562] --> Prediction: 26248.22265625\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25967.03710938 25915.98242188 25542.95117188 25332.84570312\n",
            " 25988.92382812 26324.69726562 26248.22265625] --> Prediction: 25845.154296875\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25915.98242188 25542.95117188 25332.84570312 25988.92382812\n",
            " 26324.69726562 26248.22265625 25845.15429688] --> Prediction: 25920.09375\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25542.95117188 25332.84570312 25988.92382812 26324.69726562\n",
            " 26248.22265625 25845.15429688 25920.09375   ] --> Prediction: 25500.728515625\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25332.84570312 25988.92382812 26324.69726562 26248.22265625\n",
            " 25845.15429688 25920.09375    25500.72851562] --> Prediction: 25395.0703125\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicing on: \n",
            " [25988.92382812 26324.69726562 26248.22265625 25845.15429688\n",
            " 25920.09375    25500.72851562 25395.0703125 ] --> Prediction: 25907.919921875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [26324.69726562 26248.22265625 25845.15429688 25920.09375\n",
            " 25500.72851562 25395.0703125  25907.91992188] --> Prediction: 26271.927734375\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [26248.22265625 25845.15429688 25920.09375    25500.72851562\n",
            " 25395.0703125  25907.91992188 26271.92773438] --> Prediction: 26086.662109375\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25845.15429688 25920.09375    25500.72851562 25395.0703125\n",
            " 25907.91992188 26271.92773438 26086.66210938] --> Prediction: 25880.7109375\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25920.09375    25500.72851562 25395.0703125  25907.91992188\n",
            " 26271.92773438 26086.66210938 25880.7109375 ] --> Prediction: 25964.66015625\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25500.72851562 25395.0703125  25907.91992188 26271.92773438\n",
            " 26086.66210938 25880.7109375  25964.66015625] --> Prediction: 25495.3515625\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicing on: \n",
            " [25395.0703125  25907.91992188 26271.92773438 26086.66210938\n",
            " 25880.7109375  25964.66015625 25495.3515625 ] --> Prediction: 25384.56640625\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25907.91992188 26271.92773438 26086.66210938 25880.7109375\n",
            " 25964.66015625 25495.3515625  25384.56640625] --> Prediction: 25583.763671875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [26271.92773438 26086.66210938 25880.7109375  25964.66015625\n",
            " 25495.3515625  25384.56640625 25583.76367188] --> Prediction: 26069.11328125\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [26086.66210938 25880.7109375  25964.66015625 25495.3515625\n",
            " 25384.56640625 25583.76367188 26069.11328125] --> Prediction: 26016.63671875\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Predicing on: \n",
            " [25880.7109375  25964.66015625 25495.3515625  25384.56640625\n",
            " 25583.76367188 26069.11328125 26016.63671875] --> Prediction: 25885.15625\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25964.66015625 25495.3515625  25384.56640625 25583.76367188\n",
            " 26069.11328125 26016.63671875 25885.15625   ] --> Prediction: 25874.369140625\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicing on: \n",
            " [25495.3515625  25384.56640625 25583.76367188 26069.11328125\n",
            " 26016.63671875 25885.15625    25874.36914062] --> Prediction: 25499.44140625\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25384.56640625 25583.76367188 26069.11328125 26016.63671875\n",
            " 25885.15625    25874.36914062 25499.44140625] --> Prediction: 25373.42578125\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25583.76367188 26069.11328125 26016.63671875 25885.15625\n",
            " 25874.36914062 25499.44140625 25373.42578125] --> Prediction: 25631.4140625\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [26069.11328125 26016.63671875 25885.15625    25874.36914062\n",
            " 25499.44140625 25373.42578125 25631.4140625 ] --> Prediction: 26041.34765625\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [26016.63671875 25885.15625    25874.36914062 25499.44140625\n",
            " 25373.42578125 25631.4140625  26041.34765625] --> Prediction: 25952.486328125\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicing on: \n",
            " [25885.15625    25874.36914062 25499.44140625 25373.42578125\n",
            " 25631.4140625  26041.34765625 25952.48632812] --> Prediction: 25765.212890625\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25874.36914062 25499.44140625 25373.42578125 25631.4140625\n",
            " 26041.34765625 25952.48632812 25765.21289062] --> Prediction: 25915.97265625\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25499.44140625 25373.42578125 25631.4140625  26041.34765625\n",
            " 25952.48632812 25765.21289062 25915.97265625] --> Prediction: 25518.017578125\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25373.42578125 25631.4140625  26041.34765625 25952.48632812\n",
            " 25765.21289062 25915.97265625 25518.01757812] --> Prediction: 25423.126953125\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25631.4140625  26041.34765625 25952.48632812 25765.21289062\n",
            " 25915.97265625 25518.01757812 25423.12695312] --> Prediction: 25598.775390625\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [26041.34765625 25952.48632812 25765.21289062 25915.97265625\n",
            " 25518.01757812 25423.12695312 25598.77539062] --> Prediction: 25716.625\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25952.48632812 25765.21289062 25915.97265625 25518.01757812\n",
            " 25423.12695312 25598.77539062 25716.625     ] --> Prediction: 25780.328125\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25765.21289062 25915.97265625 25518.01757812 25423.12695312\n",
            " 25598.77539062 25716.625      25780.328125  ] --> Prediction: 25713.337890625\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicing on: \n",
            " [25915.97265625 25518.01757812 25423.12695312 25598.77539062\n",
            " 25716.625      25780.328125   25713.33789062] --> Prediction: 25893.490234375\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25518.01757812 25423.12695312 25598.77539062 25716.625\n",
            " 25780.328125   25713.33789062 25893.49023438] --> Prediction: 25449.333984375\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25423.12695312 25598.77539062 25716.625      25780.328125\n",
            " 25713.33789062 25893.49023438 25449.33398438] --> Prediction: 25441.375\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25598.77539062 25716.625      25780.328125   25713.33789062\n",
            " 25893.49023438 25449.33398438 25441.375     ] --> Prediction: 25559.87890625\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25716.625      25780.328125   25713.33789062 25893.49023438\n",
            " 25449.33398438 25441.375      25559.87890625] --> Prediction: 25719.63671875\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25780.328125   25713.33789062 25893.49023438 25449.33398438\n",
            " 25441.375      25559.87890625 25719.63671875] --> Prediction: 25775.955078125\n",
            "\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Predicing on: \n",
            " [25713.33789062 25893.49023438 25449.33398438 25441.375\n",
            " 25559.87890625 25719.63671875 25775.95507812] --> Prediction: 25664.427734375\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicing on: \n",
            " [25893.49023438 25449.33398438 25441.375      25559.87890625\n",
            " 25719.63671875 25775.95507812 25664.42773438] --> Prediction: 25767.365234375\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25449.33398438 25441.375      25559.87890625 25719.63671875\n",
            " 25775.95507812 25664.42773438 25767.36523438] --> Prediction: 25469.5625\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25441.375      25559.87890625 25719.63671875 25775.95507812\n",
            " 25664.42773438 25767.36523438 25469.5625    ] --> Prediction: 25459.8203125\n",
            "\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Predicing on: \n",
            " [25559.87890625 25719.63671875 25775.95507812 25664.42773438\n",
            " 25767.36523438 25469.5625     25459.8203125 ] --> Prediction: 25594.4765625\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25719.63671875 25775.95507812 25664.42773438 25767.36523438\n",
            " 25469.5625     25459.8203125  25594.4765625 ] --> Prediction: 25657.015625\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicing on: \n",
            " [25775.95507812 25664.42773438 25767.36523438 25469.5625\n",
            " 25459.8203125  25594.4765625  25657.015625  ] --> Prediction: 25446.82421875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25664.42773438 25767.36523438 25469.5625     25459.8203125\n",
            " 25594.4765625  25657.015625   25446.82421875] --> Prediction: 25553.416015625\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25767.36523438 25469.5625     25459.8203125  25594.4765625\n",
            " 25657.015625   25446.82421875 25553.41601562] --> Prediction: 25781.044921875\n",
            "\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Predicing on: \n",
            " [25469.5625     25459.8203125  25594.4765625  25657.015625\n",
            " 25446.82421875 25553.41601562 25781.04492188] --> Prediction: 25474.072265625\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25459.8203125  25594.4765625  25657.015625   25446.82421875\n",
            " 25553.41601562 25781.04492188 25474.07226562] --> Prediction: 25376.41796875\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicing on: \n",
            " [25594.4765625  25657.015625   25446.82421875 25553.41601562\n",
            " 25781.04492188 25474.07226562 25376.41796875] --> Prediction: 25563.892578125\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25657.015625   25446.82421875 25553.41601562 25781.04492188\n",
            " 25474.07226562 25376.41796875 25563.89257812] --> Prediction: 25610.9765625\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25446.82421875 25553.41601562 25781.04492188 25474.07226562\n",
            " 25376.41796875 25563.89257812 25610.9765625 ] --> Prediction: 25467.248046875\n",
            "\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Predicing on: \n",
            " [25553.41601562 25781.04492188 25474.07226562 25376.41796875\n",
            " 25563.89257812 25610.9765625  25467.24804688] --> Prediction: 25552.314453125\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25781.04492188 25474.07226562 25376.41796875 25563.89257812\n",
            " 25610.9765625  25467.24804688 25552.31445312] --> Prediction: 25732.349609375\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicing on: \n",
            " [25474.07226562 25376.41796875 25563.89257812 25610.9765625\n",
            " 25467.24804688 25552.31445312 25732.34960938] --> Prediction: 25356.650390625\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25376.41796875 25563.89257812 25610.9765625  25467.24804688\n",
            " 25552.31445312 25732.34960938 25356.65039062] --> Prediction: 25355.17578125\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25563.89257812 25610.9765625  25467.24804688 25552.31445312\n",
            " 25732.34960938 25356.65039062 25355.17578125] --> Prediction: 25620.908203125\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25610.9765625  25467.24804688 25552.31445312 25732.34960938\n",
            " 25356.65039062 25355.17578125 25620.90820312] --> Prediction: 25605.21875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25467.24804688 25552.31445312 25732.34960938 25356.65039062\n",
            " 25355.17578125 25620.90820312 25605.21875   ] --> Prediction: 25427.619140625\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25552.31445312 25732.34960938 25356.65039062 25355.17578125\n",
            " 25620.90820312 25605.21875    25427.61914062] --> Prediction: 25225.65234375\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25732.34960938 25356.65039062 25355.17578125 25620.90820312\n",
            " 25605.21875    25427.61914062 25225.65234375] --> Prediction: 25594.2109375\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25356.65039062 25355.17578125 25620.90820312 25605.21875\n",
            " 25427.61914062 25225.65234375 25594.2109375 ] --> Prediction: 25358.1875\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25355.17578125 25620.90820312 25605.21875    25427.61914062\n",
            " 25225.65234375 25594.2109375  25358.1875    ] --> Prediction: 25391.35546875\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25620.90820312 25605.21875    25427.61914062 25225.65234375\n",
            " 25594.2109375  25358.1875     25391.35546875] --> Prediction: 25498.255859375\n",
            "\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Predicing on: \n",
            " [25605.21875    25427.61914062 25225.65234375 25594.2109375\n",
            " 25358.1875     25391.35546875 25498.25585938] --> Prediction: 25593.708984375\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25427.61914062 25225.65234375 25594.2109375  25358.1875\n",
            " 25391.35546875 25498.25585938 25593.70898438] --> Prediction: 25407.13671875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25225.65234375 25594.2109375  25358.1875     25391.35546875\n",
            " 25498.25585938 25593.70898438 25407.13671875] --> Prediction: 25286.3671875\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicing on: \n",
            " [25594.2109375  25358.1875     25391.35546875 25498.25585938\n",
            " 25593.70898438 25407.13671875 25286.3671875 ] --> Prediction: 25599.291015625\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25358.1875     25391.35546875 25498.25585938 25593.70898438\n",
            " 25407.13671875 25286.3671875  25599.29101562] --> Prediction: 25300.123046875\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicing on: \n",
            " [25391.35546875 25498.25585938 25593.70898438 25407.13671875\n",
            " 25286.3671875  25599.29101562 25300.12304688] --> Prediction: 25273.54296875\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicing on: \n",
            " [25498.25585938 25593.70898438 25407.13671875 25286.3671875\n",
            " 25599.29101562 25300.12304688 25273.54296875] --> Prediction: 25494.73046875\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25593.70898438 25407.13671875 25286.3671875  25599.29101562\n",
            " 25300.12304688 25273.54296875 25494.73046875] --> Prediction: 25621.9296875\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25407.13671875 25286.3671875  25599.29101562 25300.12304688\n",
            " 25273.54296875 25494.73046875 25621.9296875 ] --> Prediction: 25432.44921875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25286.3671875  25599.29101562 25300.12304688 25273.54296875\n",
            " 25494.73046875 25621.9296875  25432.44921875] --> Prediction: 25271.451171875\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25599.29101562 25300.12304688 25273.54296875 25494.73046875\n",
            " 25621.9296875  25432.44921875 25271.45117188] --> Prediction: 25254.408203125\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicing on: \n",
            " [25300.12304688 25273.54296875 25494.73046875 25621.9296875\n",
            " 25432.44921875 25271.45117188 25254.40820312] --> Prediction: 25193.84375\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25273.54296875 25494.73046875 25621.9296875  25432.44921875\n",
            " 25271.45117188 25254.40820312 25193.84375   ] --> Prediction: 25295.263671875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25494.73046875 25621.9296875  25432.44921875 25271.45117188\n",
            " 25254.40820312 25193.84375    25295.26367188] --> Prediction: 25491.60546875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25621.9296875  25432.44921875 25271.45117188 25254.40820312\n",
            " 25193.84375    25295.26367188 25491.60546875] --> Prediction: 25475.62109375\n",
            "\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Predicing on: \n",
            " [25432.44921875 25271.45117188 25254.40820312 25193.84375\n",
            " 25295.26367188 25491.60546875 25475.62109375] --> Prediction: 25425.71484375\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25271.45117188 25254.40820312 25193.84375    25295.26367188\n",
            " 25491.60546875 25475.62109375 25425.71484375] --> Prediction: 25243.408203125\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicing on: \n",
            " [25254.40820312 25193.84375    25295.26367188 25491.60546875\n",
            " 25475.62109375 25425.71484375 25243.40820312] --> Prediction: 25285.8046875\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25193.84375    25295.26367188 25491.60546875 25475.62109375\n",
            " 25425.71484375 25243.40820312 25285.8046875 ] --> Prediction: 25150.337890625\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25295.26367188 25491.60546875 25475.62109375 25425.71484375\n",
            " 25243.40820312 25285.8046875  25150.33789062] --> Prediction: 25253.9140625\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicing on: \n",
            " [25491.60546875 25475.62109375 25425.71484375 25243.40820312\n",
            " 25285.8046875  25150.33789062 25253.9140625 ] --> Prediction: 25347.767578125\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicing on: \n",
            " [25475.62109375 25425.71484375 25243.40820312 25285.8046875\n",
            " 25150.33789062 25253.9140625  25347.76757812] --> Prediction: 25456.57421875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25425.71484375 25243.40820312 25285.8046875  25150.33789062\n",
            " 25253.9140625  25347.76757812 25456.57421875] --> Prediction: 25460.150390625\n",
            "\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Predicing on: \n",
            " [25243.40820312 25285.8046875  25150.33789062 25253.9140625\n",
            " 25347.76757812 25456.57421875 25460.15039062] --> Prediction: 25298.7421875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25285.8046875  25150.33789062 25253.9140625  25347.76757812\n",
            " 25456.57421875 25460.15039062 25298.7421875 ] --> Prediction: 25243.7421875\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25150.33789062 25253.9140625  25347.76757812 25456.57421875\n",
            " 25460.15039062 25298.7421875  25243.7421875 ] --> Prediction: 24826.0703125\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25253.9140625  25347.76757812 25456.57421875 25460.15039062\n",
            " 25298.7421875  25243.7421875  24826.0703125 ] --> Prediction: 25120.01171875\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicing on: \n",
            " [25347.76757812 25456.57421875 25460.15039062 25298.7421875\n",
            " 25243.7421875  24826.0703125  25120.01171875] --> Prediction: 25373.529296875\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicing on: \n",
            " [25456.57421875 25460.15039062 25298.7421875  25243.7421875\n",
            " 24826.0703125  25120.01171875 25373.52929688] --> Prediction: 25469.875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25460.15039062 25298.7421875  25243.7421875  24826.0703125\n",
            " 25120.01171875 25373.52929688 25469.875     ] --> Prediction: 25298.3984375\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25298.7421875  25243.7421875  24826.0703125  25120.01171875\n",
            " 25373.52929688 25469.875      25298.3984375 ] --> Prediction: 25293.125\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicing on: \n",
            " [25243.7421875  24826.0703125  25120.01171875 25373.52929688\n",
            " 25469.875      25298.3984375  25293.125     ] --> Prediction: 25202.99609375\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [24826.0703125  25120.01171875 25373.52929688 25469.875\n",
            " 25298.3984375  25293.125      25202.99609375] --> Prediction: 24896.220703125\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicing on: \n",
            " [25120.01171875 25373.52929688 25469.875      25298.3984375\n",
            " 25293.125      25202.99609375 24896.22070312] --> Prediction: 25063.46875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25373.52929688 25469.875      25298.3984375  25293.125\n",
            " 25202.99609375 24896.22070312 25063.46875   ] --> Prediction: 25334.158203125\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25469.875      25298.3984375  25293.125      25202.99609375\n",
            " 24896.22070312 25063.46875    25334.15820312] --> Prediction: 25309.771484375\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25298.3984375  25293.125      25202.99609375 24896.22070312\n",
            " 25063.46875    25334.15820312 25309.77148438] --> Prediction: 25272.759765625\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25293.125      25202.99609375 24896.22070312 25063.46875\n",
            " 25334.15820312 25309.77148438 25272.75976562] --> Prediction: 25346.796875\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25202.99609375 24896.22070312 25063.46875    25334.15820312\n",
            " 25309.77148438 25272.75976562 25346.796875  ] --> Prediction: 25214.693359375\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [24896.22070312 25063.46875    25334.15820312 25309.77148438\n",
            " 25272.75976562 25346.796875   25214.69335938] --> Prediction: 24865.87109375\n",
            "\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Predicing on: \n",
            " [25063.46875    25334.15820312 25309.77148438 25272.75976562\n",
            " 25346.796875   25214.69335938 24865.87109375] --> Prediction: 24746.8125\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25334.15820312 25309.77148438 25272.75976562 25346.796875\n",
            " 25214.69335938 24865.87109375 24746.8125    ] --> Prediction: 25174.984375\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25309.77148438 25272.75976562 25346.796875   25214.69335938\n",
            " 24865.87109375 24746.8125     25174.984375  ] --> Prediction: 25304.6328125\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25272.75976562 25346.796875   25214.69335938 24865.87109375\n",
            " 24746.8125     25174.984375   25304.6328125 ] --> Prediction: 25302.642578125\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25346.796875   25214.69335938 24865.87109375 24746.8125\n",
            " 25174.984375   25304.6328125  25302.64257812] --> Prediction: 25214.828125\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicing on: \n",
            " [25214.69335938 24865.87109375 24746.8125     25174.984375\n",
            " 25304.6328125  25302.64257812 25214.828125  ] --> Prediction: 25230.28125\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [24865.87109375 24746.8125     25174.984375   25304.6328125\n",
            " 25302.64257812 25214.828125   25230.28125   ] --> Prediction: 24841.62109375\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [24746.8125     25174.984375   25304.6328125  25302.64257812\n",
            " 25214.828125   25230.28125    24841.62109375] --> Prediction: 24820.982421875\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25174.984375   25304.6328125  25302.64257812 25214.828125\n",
            " 25230.28125    24841.62109375 24820.98242188] --> Prediction: 25120.513671875\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25304.6328125  25302.64257812 25214.828125   25230.28125\n",
            " 24841.62109375 24820.98242188 25120.51367188] --> Prediction: 25252.224609375\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25302.64257812 25214.828125   25230.28125    24841.62109375\n",
            " 24820.98242188 25120.51367188 25252.22460938] --> Prediction: 25169.32421875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25214.828125   25230.28125    24841.62109375 24820.98242188\n",
            " 25120.51367188 25252.22460938 25169.32421875] --> Prediction: 25239.25\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25230.28125    24841.62109375 24820.98242188 25120.51367188\n",
            " 25252.22460938 25169.32421875 25239.25      ] --> Prediction: 25265.6015625\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [24841.62109375 24820.98242188 25120.51367188 25252.22460938\n",
            " 25169.32421875 25239.25       25265.6015625 ] --> Prediction: 24879.51953125\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [24820.98242188 25120.51367188 25252.22460938 25169.32421875\n",
            " 25239.25       25265.6015625  24879.51953125] --> Prediction: 24801.197265625\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25120.51367188 25252.22460938 25169.32421875 25239.25\n",
            " 25265.6015625  24879.51953125 24801.19726562] --> Prediction: 24796.091796875\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25252.22460938 25169.32421875 25239.25       25265.6015625\n",
            " 24879.51953125 24801.19726562 24796.09179688] --> Prediction: 25087.0\n",
            "\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Predicing on: \n",
            " [25169.32421875 25239.25       25265.6015625  24879.51953125\n",
            " 24801.19726562 24796.09179688 25087.        ] --> Prediction: 25138.75390625\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25239.25       25265.6015625  24879.51953125 24801.19726562\n",
            " 24796.09179688 25087.         25138.75390625] --> Prediction: 25248.90234375\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25265.6015625  24879.51953125 24801.19726562 24796.09179688\n",
            " 25087.         25138.75390625 25248.90234375] --> Prediction: 25130.287109375\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [24879.51953125 24801.19726562 24796.09179688 25087.\n",
            " 25138.75390625 25248.90234375 25130.28710938] --> Prediction: 24898.13671875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [24801.19726562 24796.09179688 25087.         25138.75390625\n",
            " 25248.90234375 25130.28710938 24898.13671875] --> Prediction: 24779.525390625\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [24796.09179688 25087.         25138.75390625 25248.90234375\n",
            " 25130.28710938 24898.13671875 24779.52539062] --> Prediction: 24842.0546875\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25087.         25138.75390625 25248.90234375 25130.28710938\n",
            " 24898.13671875 24779.52539062 24842.0546875 ] --> Prediction: 25047.1640625\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25138.75390625 25248.90234375 25130.28710938 24898.13671875\n",
            " 24779.52539062 24842.0546875  25047.1640625 ] --> Prediction: 25089.259765625\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicing on: \n",
            " [25248.90234375 25130.28710938 24898.13671875 24779.52539062\n",
            " 24842.0546875  25047.1640625  25089.25976562] --> Prediction: 25124.82421875\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25130.28710938 24898.13671875 24779.52539062 24842.0546875\n",
            " 25047.1640625  25089.25976562 25124.82421875] --> Prediction: 25139.4921875\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [24898.13671875 24779.52539062 24842.0546875  25047.1640625\n",
            " 25089.25976562 25124.82421875 25139.4921875 ] --> Prediction: 24925.177734375\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [24779.52539062 24842.0546875  25047.1640625  25089.25976562\n",
            " 25124.82421875 25139.4921875  24925.17773438] --> Prediction: 24823.26953125\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [24842.0546875  25047.1640625  25089.25976562 25124.82421875\n",
            " 25139.4921875  24925.17773438 24823.26953125] --> Prediction: 24806.693359375\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25047.1640625  25089.25976562 25124.82421875 25139.4921875\n",
            " 24925.17773438 24823.26953125 24806.69335938] --> Prediction: 24721.9140625\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25089.25976562 25124.82421875 25139.4921875  24925.17773438\n",
            " 24823.26953125 24806.69335938 24721.9140625 ] --> Prediction: 24936.146484375\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25124.82421875 25139.4921875  24925.17773438 24823.26953125\n",
            " 24806.69335938 24721.9140625  24936.14648438] --> Prediction: 25103.029296875\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25139.4921875  24925.17773438 24823.26953125 24806.69335938\n",
            " 24721.9140625  24936.14648438 25103.02929688] --> Prediction: 25138.544921875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [24925.17773438 24823.26953125 24806.69335938 24721.9140625\n",
            " 24936.14648438 25103.02929688 25138.54492188] --> Prediction: 24829.7421875\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicing on: \n",
            " [24823.26953125 24806.69335938 24721.9140625  24936.14648438\n",
            " 25103.02929688 25138.54492188 24829.7421875 ] --> Prediction: 24842.19921875\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [24806.69335938 24721.9140625  24936.14648438 25103.02929688\n",
            " 25138.54492188 24829.7421875  24842.19921875] --> Prediction: 24771.64453125\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [24721.9140625  24936.14648438 25103.02929688 25138.54492188\n",
            " 24829.7421875  24842.19921875 24771.64453125] --> Prediction: 24759.810546875\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [24936.14648438 25103.02929688 25138.54492188 24829.7421875\n",
            " 24842.19921875 24771.64453125 24759.81054688] --> Prediction: 24899.302734375\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [25103.02929688 25138.54492188 24829.7421875  24842.19921875\n",
            " 24771.64453125 24759.81054688 24899.30273438] --> Prediction: 25057.1328125\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25138.54492188 24829.7421875  24842.19921875 24771.64453125\n",
            " 24759.81054688 24899.30273438 25057.1328125 ] --> Prediction: 25011.826171875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [24829.7421875  24842.19921875 24771.64453125 24759.81054688\n",
            " 24899.30273438 25057.1328125  25011.82617188] --> Prediction: 24837.005859375\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [24842.19921875 24771.64453125 24759.81054688 24899.30273438\n",
            " 25057.1328125  25011.82617188 24837.00585938] --> Prediction: 24878.765625\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicing on: \n",
            " [24771.64453125 24759.81054688 24899.30273438 25057.1328125\n",
            " 25011.82617188 24837.00585938 24878.765625  ] --> Prediction: 24800.2421875\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicing on: \n",
            " [24759.81054688 24899.30273438 25057.1328125  25011.82617188\n",
            " 24837.00585938 24878.765625   24800.2421875 ] --> Prediction: 24714.03125\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [24899.30273438 25057.1328125  25011.82617188 24837.00585938\n",
            " 24878.765625   24800.2421875  24714.03125   ] --> Prediction: 24583.48828125\n",
            "\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Predicing on: \n",
            " [25057.1328125  25011.82617188 24837.00585938 24878.765625\n",
            " 24800.2421875  24714.03125    24583.48828125] --> Prediction: 24912.98828125\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25011.82617188 24837.00585938 24878.765625   24800.2421875\n",
            " 24714.03125    24583.48828125 24912.98828125] --> Prediction: 25006.482421875\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [24837.00585938 24878.765625   24800.2421875  24714.03125\n",
            " 24583.48828125 24912.98828125 25006.48242188] --> Prediction: 24857.99609375\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [24878.765625   24800.2421875  24714.03125    24583.48828125\n",
            " 24912.98828125 25006.48242188 24857.99609375] --> Prediction: 24793.43359375\n",
            "\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Predicing on: \n",
            " [24800.2421875  24714.03125    24583.48828125 24912.98828125\n",
            " 25006.48242188 24857.99609375 24793.43359375] --> Prediction: 24802.4921875\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [24714.03125    24583.48828125 24912.98828125 25006.48242188\n",
            " 24857.99609375 24793.43359375 24802.4921875 ] --> Prediction: 24687.326171875\n",
            "\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Predicing on: \n",
            " [24583.48828125 24912.98828125 25006.48242188 24857.99609375\n",
            " 24793.43359375 24802.4921875  24687.32617188] --> Prediction: 24632.623046875\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicing on: \n",
            " [24912.98828125 25006.48242188 24857.99609375 24793.43359375\n",
            " 24802.4921875  24687.32617188 24632.62304688] --> Prediction: 24895.83203125\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Predicing on: \n",
            " [25006.48242188 24857.99609375 24793.43359375 24802.4921875\n",
            " 24687.32617188 24632.62304688 24895.83203125] --> Prediction: 24953.134765625\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicing on: \n",
            " [24857.99609375 24793.43359375 24802.4921875  24687.32617188\n",
            " 24632.62304688 24895.83203125 24953.13476562] --> Prediction: 24750.349609375\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Predicing on: \n",
            " [24793.43359375 24802.4921875  24687.32617188 24632.62304688\n",
            " 24895.83203125 24953.13476562 24750.34960938] --> Prediction: 24799.64453125\n",
            "\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Predicing on: \n",
            " [24802.4921875  24687.32617188 24632.62304688 24895.83203125\n",
            " 24953.13476562 24750.34960938 24799.64453125] --> Prediction: 24842.09375\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Predicing on: \n",
            " [24687.32617188 24632.62304688 24895.83203125 24953.13476562\n",
            " 24750.34960938 24799.64453125 24842.09375   ] --> Prediction: 24706.93359375\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicing on: \n",
            " [24632.62304688 24895.83203125 24953.13476562 24750.34960938\n",
            " 24799.64453125 24842.09375    24706.93359375] --> Prediction: 24598.736328125\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicing on: \n",
            " [24895.83203125 24953.13476562 24750.34960938 24799.64453125\n",
            " 24842.09375    24706.93359375 24598.73632812] --> Prediction: 24572.412109375\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Predicing on: \n",
            " [24953.13476562 24750.34960938 24799.64453125 24842.09375\n",
            " 24706.93359375 24598.73632812 24572.41210938] --> Prediction: 24807.67578125\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicing on: \n",
            " [24750.34960938 24799.64453125 24842.09375    24706.93359375\n",
            " 24598.73632812 24572.41210938 24807.67578125] --> Prediction: 24739.6875\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Predicing on: \n",
            " [24799.64453125 24842.09375    24706.93359375 24598.73632812\n",
            " 24572.41210938 24807.67578125 24739.6875    ] --> Prediction: 24813.55078125\n",
            "\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Predicing on: \n",
            " [24842.09375    24706.93359375 24598.73632812 24572.41210938\n",
            " 24807.67578125 24739.6875     24813.55078125] --> Prediction: 24738.029296875\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Predicing on: \n",
            " [24706.93359375 24598.73632812 24572.41210938 24807.67578125\n",
            " 24739.6875     24813.55078125 24738.02929688] --> Prediction: 24711.5625\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Predicing on: \n",
            " [24598.73632812 24572.41210938 24807.67578125 24739.6875\n",
            " 24813.55078125 24738.02929688 24711.5625    ] --> Prediction: 24578.15625\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicing on: \n",
            " [24572.41210938 24807.67578125 24739.6875     24813.55078125\n",
            " 24738.02929688 24711.5625     24578.15625   ] --> Prediction: 24613.8671875\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicing on: \n",
            " [24807.67578125 24739.6875     24813.55078125 24738.02929688\n",
            " 24711.5625     24578.15625    24613.8671875 ] --> Prediction: 24792.076171875\n",
            "\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Predicing on: \n",
            " [24739.6875     24813.55078125 24738.02929688 24711.5625\n",
            " 24578.15625    24613.8671875  24792.07617188] --> Prediction: 24687.994140625\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicing on: \n",
            " [24813.55078125 24738.02929688 24711.5625     24578.15625\n",
            " 24613.8671875  24792.07617188 24687.99414062] --> Prediction: 24694.46484375\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicing on: \n",
            " [24738.02929688 24711.5625     24578.15625    24613.8671875\n",
            " 24792.07617188 24687.99414062 24694.46484375] --> Prediction: 24751.35546875\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Predicing on: \n",
            " [24711.5625     24578.15625    24613.8671875  24792.07617188\n",
            " 24687.99414062 24694.46484375 24751.35546875] --> Prediction: 24737.2421875\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicing on: \n",
            " [24578.15625    24613.8671875  24792.07617188 24687.99414062\n",
            " 24694.46484375 24751.35546875 24737.2421875 ] --> Prediction: 24616.306640625\n",
            "\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Predicing on: \n",
            " [24613.8671875  24792.07617188 24687.99414062 24694.46484375\n",
            " 24751.35546875 24737.2421875  24616.30664062] --> Prediction: 24581.98046875\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Predicing on: \n",
            " [24792.07617188 24687.99414062 24694.46484375 24751.35546875\n",
            " 24737.2421875  24616.30664062 24581.98046875] --> Prediction: 24466.0078125\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Predicing on: \n",
            " [24687.99414062 24694.46484375 24751.35546875 24737.2421875\n",
            " 24616.30664062 24581.98046875 24466.0078125 ] --> Prediction: 24565.369140625\n",
            "\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Predicing on: \n",
            " [24694.46484375 24751.35546875 24737.2421875  24616.30664062\n",
            " 24581.98046875 24466.0078125  24565.36914062] --> Prediction: 24701.66796875\n",
            "\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Predicing on: \n",
            " [24751.35546875 24737.2421875  24616.30664062 24581.98046875\n",
            " 24466.0078125  24565.36914062 24701.66796875] --> Prediction: 24752.51953125\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Predicing on: \n",
            " [24737.2421875  24616.30664062 24581.98046875 24466.0078125\n",
            " 24565.36914062 24701.66796875 24752.51953125] --> Prediction: 24623.64453125\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Predicing on: \n",
            " [24616.30664062 24581.98046875 24466.0078125  24565.36914062\n",
            " 24701.66796875 24752.51953125 24623.64453125] --> Prediction: 24616.775390625\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Predicing on: \n",
            " [24581.98046875 24466.0078125  24565.36914062 24701.66796875\n",
            " 24752.51953125 24623.64453125 24616.77539062] --> Prediction: 24549.68359375\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicing on: \n",
            " [24466.0078125  24565.36914062 24701.66796875 24752.51953125\n",
            " 24623.64453125 24616.77539062 24549.68359375] --> Prediction: 24500.8203125\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicing on: \n",
            " [24565.36914062 24701.66796875 24752.51953125 24623.64453125\n",
            " 24616.77539062 24549.68359375 24500.8203125 ] --> Prediction: 24533.98828125\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicing on: \n",
            " [24701.66796875 24752.51953125 24623.64453125 24616.77539062\n",
            " 24549.68359375 24500.8203125  24533.98828125] --> Prediction: 24658.806640625\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. For future predictions, re-train the model each time a new prediction is made"
      ],
      "metadata": {
        "id": "Nm5jnxUzCI7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- append pred to list -> train new model\n",
        "- make a pred on new list (with new model)\n",
        "- append new pred to list -> train new model\n",
        "- make a pred on new list (with new model)\n",
        "- append new pred to list -> train new model\n",
        "- make a pred on new list (with new model)\n",
        "- #7 is train a new model every single time a pred is made"
      ],
      "metadata": {
        "id": "b2ig5nQbCUMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets code things at first without a loop and see how it foes\n",
        "HORIZON = 1\n",
        "WINDOW_SIZE = 7\n",
        "\n",
        "# Building a model (You can replace it with any model)\n",
        "def get_model(horizon = HORIZON):\n",
        "    model = tf.keras.Sequential([\n",
        "            layers.Dense(128 , activation = 'relu'),\n",
        "            layers.Dense(128 , activation = 'relu'),\n",
        "            layers.Dense(horizon)\n",
        "        ])\n",
        "\n",
        "    model.compile(loss = tf.keras.losses.mae ,\n",
        "                  optimizer = tf.keras.optimizers.Adam())\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "NIEZPLoxCTJR"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#. Making the data and labels for window size of 7 and horizon of 1\n",
        "full_windows , full_labels = make_windows(prices , window_size= WINDOW_SIZE , horizon= HORIZON)"
      ],
      "metadata": {
        "id": "p9hZTjf4CkHO"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making future forecasts of Bitcoin (using the whole data)\n",
        "def pred_model_run(values , X, model , into_future , window_size  , horizon, epochs ):\n",
        "\n",
        "  '''\n",
        "  This function train a model for every updated predictions.\n",
        "\n",
        "  Arguments:\n",
        "  ----------\n",
        "      - values --> labels / truth values. Bitcoin prices\n",
        "      - X --> Windowed data of the bitcoin prices (default window size is 7)\n",
        "      - model --> compiled model with default horizon 1\n",
        "      - into_future -->  how many time steps to predict in the future?\n",
        "      - window_size --> default is 7 (using the 7 days prices of bitcoin)\n",
        "      - horizon --> default is 1 (predicting the price of next day)\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "      - model --> a model that has been trained on all the previous predictions + the data\n",
        "  '''\n",
        "\n",
        "  last_window = values[-window_size:]\n",
        "  X_all = X\n",
        "  y_all = values\n",
        "  for _ in range(into_future):\n",
        "\n",
        "      # Each time the model is trained for 5 epochs with the updated data\n",
        "      model.fit(x = X_all , y = y_all , epochs = epochs , verbose = 0)\n",
        "\n",
        "      future_pred = model.predict(tf.expand_dims(last_window, axis= 0))\n",
        "      #future_pred = model.predict(last_window)\n",
        "      print(f'Predicing on: \\n {last_window} --> Prediction: {tf.squeeze(future_pred).numpy()}\\n')\n",
        "\n",
        "      future_forecast.append(tf.squeeze(future_pred).numpy())\n",
        "      #values = np.append(values , tf.squeeze(future_pred).numpy())\n",
        "      for i in range(0 , len(X_all)):\n",
        "        x = X_all[i][1:]  # removing the 0th index of the X window ()\n",
        "        y = y_all[1:] # removing the 0th index  of y\n",
        "        X = np.append(x , future_pred) # append the future pred at last to X window\n",
        "        values = np.append(y , future_pred) # appending the future pred to y\n",
        "\n",
        "      # Update the last window\n",
        "      last_window = np.append(last_window , future_pred)[-window_size:]\n",
        "\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "0D6IcZ9UClnz"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_windows.shape , X_all.shape , full_labels.shape , y_all.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9wdFBd2C_7d",
        "outputId": "bdaa561e-1624-4cdb-c945-fc9661a2f443"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3536, 7), (3536, 7), (3536, 1), (3536,))"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the above function\n",
        "trained_model = pred_model_run(values = tf.squeeze(full_labels) ,\n",
        "                               X = full_windows ,\n",
        "                               model = get_model(horizon = 1) ,\n",
        "                               window_size = WINDOW_SIZE ,\n",
        "                               horizon = HORIZON ,\n",
        "                               epochs = 10 ,\n",
        "                               into_future  =14 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9bK0AExDAWa",
        "outputId": "e0a9d878-4e45-4d09-c61d-64cada2a7dc5"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 46ms/step\n",
            "Predicing on: \n",
            " [26359.59 26493.46 26489.46 25842.84 25920.86 25887.59 25910.03] --> Prediction: 26244.83984375\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicing on: \n",
            " [26493.46       26489.46       25842.84       25920.86\n",
            " 25887.59       25910.03       26244.83984375] --> Prediction: 26202.529296875\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [26489.46       25842.84       25920.86       25887.59\n",
            " 25910.03       26244.83984375 26202.52929688] --> Prediction: 26946.435546875\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25842.84       25920.86       25887.59       25910.03\n",
            " 26244.83984375 26202.52929688 26946.43554688] --> Prediction: 27313.15234375\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [25920.86       25887.59       25910.03       26244.83984375\n",
            " 26202.52929688 26946.43554688 27313.15234375] --> Prediction: 27862.380859375\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [25887.59       25910.03       26244.83984375 26202.52929688\n",
            " 26946.43554688 27313.15234375 27862.38085938] --> Prediction: 28435.666015625\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicing on: \n",
            " [25910.03       26244.83984375 26202.52929688 26946.43554688\n",
            " 27313.15234375 27862.38085938 28435.66601562] --> Prediction: 28762.189453125\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [26244.83984375 26202.52929688 26946.43554688 27313.15234375\n",
            " 27862.38085938 28435.66601562 28762.18945312] --> Prediction: 29177.857421875\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicing on: \n",
            " [26202.52929688 26946.43554688 27313.15234375 27862.38085938\n",
            " 28435.66601562 28762.18945312 29177.85742188] --> Prediction: 29588.376953125\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicing on: \n",
            " [26946.43554688 27313.15234375 27862.38085938 28435.66601562\n",
            " 28762.18945312 29177.85742188 29588.37695312] --> Prediction: 30087.71875\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Predicing on: \n",
            " [27313.15234375 27862.38085938 28435.66601562 28762.18945312\n",
            " 29177.85742188 29588.37695312 30087.71875   ] --> Prediction: 30231.416015625\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [27862.38085938 28435.66601562 28762.18945312 29177.85742188\n",
            " 29588.37695312 30087.71875    30231.41601562] --> Prediction: 30427.349609375\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicing on: \n",
            " [28435.66601562 28762.18945312 29177.85742188 29588.37695312\n",
            " 30087.71875    30231.41601562 30427.34960938] --> Prediction: 30961.1953125\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Predicing on: \n",
            " [28762.18945312 29177.85742188 29588.37695312 30087.71875\n",
            " 30231.41601562 30427.34960938 30961.1953125 ] --> Prediction: 31416.51953125\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Using a purpose-build forecasting algorithm (Facebook Kat's Library)"
      ],
      "metadata": {
        "id": "vjgThMWrDCuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try out one of the extra algorithms listed in the modelling experiments part such as:\n",
        "\n",
        "[Facebook’s Kats library](https://github.com/facebookresearch/Kats) - there are many models in here\n",
        "\n",
        "[LinkedIn’s Greykite library](https://github.com/linkedin/greykite)"
      ],
      "metadata": {
        "id": "V8UkHQLaDc0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing facebooks kats lib\n",
        "!pip install kats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD56Yu8_DXCc",
        "outputId": "ead5c62c-393d-4c02-cc87-8252e9af7e56"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kats\n",
            "  Using cached kats-0.2.0-py3-none-any.whl (612 kB)\n",
            "Requirement already satisfied: attrs>=21.2.0 in /usr/local/lib/python3.10/dist-packages (from kats) (23.1.0)\n",
            "Collecting deprecated>=1.2.12 (from kats)\n",
            "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from kats) (3.7.1)\n",
            "Collecting numpy<1.22,>=1.21 (from kats)\n",
            "  Using cached numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "Collecting pandas<=1.3.5,>=1.0.4 (from kats)\n",
            "  Using cached pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from kats) (2.8.2)\n",
            "Collecting pystan==2.19.1.1 (from kats)\n",
            "  Using cached pystan-2.19.1.1.tar.gz (16.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fbprophet==0.7.1 (from kats)\n",
            "  Using cached fbprophet-0.7.1.tar.gz (64 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from kats) (1.2.2)\n",
            "Collecting scipy<1.8.0 (from kats)\n",
            "  Using cached scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9 MB)\n",
            "Requirement already satisfied: seaborn>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from kats) (0.12.2)\n",
            "Collecting setuptools-git>=1.2 (from kats)\n",
            "  Using cached setuptools_git-1.2-py2.py3-none-any.whl (10 kB)\n",
            "Collecting statsmodels==0.12.2 (from kats)\n",
            "  Using cached statsmodels-0.12.2.tar.gz (17.5 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the TimeSerisData class from Kats\n",
        "from kats.consts import TimeSeriesData"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "RKtoOHpYDq_m",
        "outputId": "0ff37630-818d-4312-a49e-f7ed8b1582c3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-171aafba5986>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Importing the TimeSerisData class from Kats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeriesData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kats'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils to work with Kats\n",
        "from dateutil import parser\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "QQ-A24rtDsjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Dataset object with Kats\n",
        "ts_data = TimeSeriesData(time = bitcoin_prices.index ,\n",
        "               value = bitcoin_prices.Price)\n",
        "\n",
        "type(ts_data)"
      ],
      "metadata": {
        "id": "EEEkwGCFDvY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the timeseries data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ts_data.plot(cols=['Price'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bb2iCiT7EMsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forecasting with the Prophet model"
      ],
      "metadata": {
        "id": "cSL84lkHEQ0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the prophet\n",
        "from kats.models.prophet import ProphetModel , ProphetParams\n",
        "\n",
        "# Creating a model param instance\n",
        "params = ProphetParams(seasonality_mode= 'multiplicative')\n",
        "\n",
        "# Create a prophet model instance (just like how we create an instance in sklearn)\n",
        "model = ProphetModel(ts_data , params)\n",
        "\n",
        "# Fitting the model\n",
        "model.fit()\n",
        "\n",
        "# Making predictions\n",
        "forecast = model.predict(steps= 1 , include_history= True , freq = '1W')"
      ],
      "metadata": {
        "id": "U6FQZ8ecEOeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the bitcoin price for a day (Horizon = 1 )\n",
        "forecast.head(10)"
      ],
      "metadata": {
        "id": "yCIh20yAEW__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble of models using Kat's library using `KatEnsemble`"
      ],
      "metadata": {
        "id": "MEOKwRocEfsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importing the things we need\n",
        "from kats.models.ensemble.ensemble import EnsembleParams , BaseModelParams\n",
        "from kats.models.ensemble.kats_ensemble import KatsEnsemble\n",
        "from kats.models import (\n",
        "    arima,\n",
        "    holtwinters ,\n",
        "    linear_model ,\n",
        "    prophet ,\n",
        "    quadratic_model ,\n",
        "    sarima ,\n",
        "    theta\n",
        ")"
      ],
      "metadata": {
        "id": "9uhCdE2yEYsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Defining the parameters of different models\n",
        "model_params = EnsembleParams(\n",
        "    [\n",
        "     BaseModelParams('arima' , arima.ARIMAParams(p = 1 , d=1 , q=1)) ,\n",
        "     BaseModelParams('sarima' ,\n",
        "                     sarima.SARIMAParams(\n",
        "                         p = 2 , d= 2 , q =1 , trend = 'ct' ,\n",
        "                     seasonal_order = (1, 0 ,1 ,12) , enforce_invertibility = False ,\n",
        "                     enforce_stationarity = False),\n",
        "     ),\n",
        "     BaseModelParams('prophet' , prophet.ProphetParams()) ,\n",
        "     BaseModelParams('linear' , linear_model.LinearModelParams()) ,\n",
        "     BaseModelParams('quadratic' , quadratic_model.QuadraticModelParams()),\n",
        "     BaseModelParams('theta' , theta.ThetaParams()),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "8cZ8tSmAEzTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creating KatEnsembleParam with detailed configuration\n",
        "KatEnsembleParams = {\n",
        "    'models': model_params ,\n",
        "    'aggregation': 'median' ,\n",
        "    'seasonality_length': 7 ,\n",
        "    'decomposition_method': 'multiplicative'\n",
        "}"
      ],
      "metadata": {
        "id": "xZ4t44MhE1_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Time Series dataset\n",
        "bitcoin_ts = TimeSeriesData(value = bitcoin_prices.Price,\n",
        "                            time = bitcoin_prices.index ,\n",
        "                            sort_by_time= True)"
      ],
      "metadata": {
        "id": "bMqEzEajE3CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a KatEnsemble model (or) instantiating it\n",
        "ensemble_model = KatsEnsemble(\n",
        "    data = bitcoin_ts ,\n",
        "    params = KatEnsembleParams\n",
        ")\n",
        "\n",
        "# Fitting the model\n",
        "ensemble_model.fit()"
      ],
      "metadata": {
        "id": "u_uZl05QE4Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making prediction for the next 30 days\n",
        "forecast = ensemble_model.predict(steps = 30)"
      ],
      "metadata": {
        "id": "hKuLzawFFDRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate individual model results (we will get the predictions for 30 Days)\n",
        "ensemble_model.aggregate()"
      ],
      "metadata": {
        "id": "Yu-N8BMaFEGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the model\n",
        "ensemble_model.plot()"
      ],
      "metadata": {
        "id": "Tsft5Dn6FFSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G2LVNBZsFGev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}